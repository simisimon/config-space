{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from cfgnet.network.network_configuration import NetworkConfiguration\n",
    "from cfgnet.network.nodes import ArtifactNode\n",
    "from cfgnet.network.network import Network\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import git\n",
    "import json\n",
    "import subprocess\n",
    "import traceback\n",
    "import glob\n",
    "import time\n",
    "\n",
    "config_file_endings = (\".xml\", \".yml\", \".yaml\", \"Dockerfile\", \".ini\", \".properties\", \".conf\", \".json\", \".toml\", \".cfg\", \"settings.py\", \".cnf\")\n",
    "\n",
    "def checkout_latest_commit(repo, current_branch, latest_commit):\n",
    "     # Return to the latest commit\n",
    "    if current_branch:\n",
    "        # If we were on a branch, return to it\n",
    "        repo.git.checkout(current_branch)\n",
    "        print(f\"Returned to original branch: {current_branch}\")\n",
    "    else:\n",
    "        # If we were in a detached HEAD state, checkout the latest commit directly\n",
    "        repo.git.checkout(latest_commit)\n",
    "        print(f\"Returned to the latest commit: {latest_commit}\")\n",
    "\n",
    "\n",
    "def analyze_config_network(repo_path: str):\n",
    "    \n",
    "    network_config = NetworkConfiguration(\n",
    "        project_root_abs=repo_path,\n",
    "        enable_static_blacklist=False,\n",
    "        enable_internal_links=True,\n",
    "        enable_all_conflicts=True,\n",
    "        enable_file_type_plugins=True,\n",
    "        system_level=False\n",
    "    )\n",
    "\n",
    "    network = Network.init_network(cfg=network_config)\n",
    "\n",
    "    artifacts = network.get_nodes(node_type=ArtifactNode)\n",
    "\n",
    "    config_files_data = []\n",
    "    for artifact in artifacts:\n",
    "        pairs = artifact.get_pairs()\n",
    "\n",
    "        # exclude file options\n",
    "        pairs = [pair for pair in pairs if pair[\"option\"] != \"file\"] \n",
    "\n",
    "        config_files_data.append({\n",
    "            \"file_path\": artifact.rel_file_path,\n",
    "            \"concept\": artifact.concept_name,\n",
    "            \"options\": len(artifact.get_pairs()),\n",
    "            \"pairs\": pairs\n",
    "        })\n",
    "\n",
    "\n",
    "    config_files = set(artifact.rel_file_path for artifact in artifacts)\n",
    "  \t\n",
    "    network_data = {\n",
    "        \"links\": len(network.links),\n",
    "        \"config_files\": list(config_files),\n",
    "        \"config_files_data\": config_files_data\n",
    "    }\n",
    "\n",
    "    return network_data\n",
    "\n",
    "\n",
    "def get_file_diff(repo_path: str, commit, file_path: str):\n",
    "    if commit.parents:\n",
    "        parent_commit = f\"{commit.hexsha}^\"\n",
    "            \n",
    "        try:                        \n",
    "            # Run git diff to capture line-by-line changes\n",
    "            diff_output = subprocess.check_output(\n",
    "                ['git', 'diff', parent_commit, commit.hexsha, '--', file_path],\n",
    "                cwd=repo_path,\n",
    "                text=True\n",
    "            )\n",
    "            return diff_output\n",
    "        except (subprocess.CalledProcessError, git.exc.GitCommandError) as e:\n",
    "            print(f\"Error running git diff for commit {commit.hexsha}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def analyze_repository(repo_path: str, get_diff: bool = False) -> Dict:\n",
    "    \"\"\"Analyze Commit history of repositories and collect stats about the configuration space.\"\"\"  \n",
    "    start_time = time.time()\n",
    "    project_name = repo_path.split(\"/\")[-1]\n",
    "    repo = git.Repo(repo_path)\n",
    "\n",
    "    # Save the current branch to return to it later\n",
    "    current_branch = repo.active_branch.name if not repo.head.is_detached else None\n",
    "    latest_commit = repo.head.commit.hexsha\n",
    "    parent_commit = None\n",
    "\n",
    "    # Get all commits in the repository from oldest to newest\n",
    "    commits = list(repo.iter_commits(\"HEAD\"))[::-1]\n",
    "\n",
    "    print(f\"Number of commits: {len(commits)}\")\n",
    "\n",
    "    config_commit_data = []\n",
    "\n",
    "    for commit in tqdm(commits, desc=\"Processing\", total=len(commits)):\n",
    "\n",
    "        is_config_related = False\n",
    "\n",
    "        # Get commit stats\n",
    "        stats = commit.stats.total\n",
    "\n",
    "        # Checkout the commit\n",
    "        repo.git.checkout(commit.hexsha)\n",
    "\n",
    "        # check if commit is config-related\n",
    "        if any(file_path.endswith(config_file_endings) for file_path in commit.stats.files.keys()):\n",
    "            is_config_related = True\n",
    "            \n",
    "            # Run the external analysis for config-related commits\n",
    "            try: \n",
    "                network_data = analyze_config_network(repo_path=repo_path)\n",
    "            except Exception:\n",
    "                print(f\"Error occurred in commit {commit.hexsha}\")\n",
    "                print({traceback.print_exc()})\n",
    "                return\n",
    "\n",
    "            # Get general stats per config file\n",
    "            for file_path, file_stats in commit.stats.files.items():\n",
    "                \n",
    "                # Get config file data\n",
    "                if file_path in network_data[\"config_files\"]:\n",
    "                    file_data = next(filter(lambda x: x[\"file_path\"] == file_path, network_data[\"config_files_data\"]))\n",
    "                    file_data[\"insertions\"] = file_stats['insertions']\n",
    "                    file_data[\"deletions\"] = file_stats['deletions']\n",
    "                    file_data[\"total_changes\"] = file_stats['insertions'] + file_stats['deletions']\n",
    "\n",
    "                    # Get config file diff\n",
    "                    if get_diff:\n",
    "                        diff_output = get_file_diff(\n",
    "                            repo_path=repo_path,\n",
    "                            commit=commit,\n",
    "                            file_path=file_path\n",
    "                        )\n",
    "\n",
    "                        file_data[\"diff\"] = diff_output\n",
    "\n",
    "            config_commit_data.append(\n",
    "                {   \n",
    "                    \"commit_hash\": str(commit.hexsha),\n",
    "                    \"parent_commit\": (parent_commit),\n",
    "                    \"is_config_related\": is_config_related,\n",
    "                    \"author\": f\"{commit.author.name} <{commit.author.email}>\",\n",
    "                    \"commit_mgs\": str(commit.message),\n",
    "                    \"files_changed\": stats['files'],\n",
    "                    \"insertions\": stats['insertions'],\n",
    "                    \"deletions\": stats['deletions'],\n",
    "                    \"network_data\": network_data\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            config_commit_data.append(\n",
    "                {   \n",
    "                    \"commit_hash\": str(commit.hexsha),\n",
    "                    \"parent_commit\": (parent_commit),\n",
    "                    \"is_config_related\": is_config_related,\n",
    "                    \"author\": f\"{commit.author.name} <{commit.author.email}>\",\n",
    "                    \"commit_mgs\": str(commit.message),\n",
    "                    \"files_changed\": stats['files'],\n",
    "                    \"insertions\": stats['insertions'],\n",
    "                    \"deletions\": stats['deletions'],\n",
    "                    \"network_data\": None\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    # Return to latest commit\n",
    "    checkout_latest_commit(\n",
    "        repo=repo, \n",
    "        current_branch=current_branch,\n",
    "        latest_commit=latest_commit\n",
    "    )\n",
    "\n",
    "    print(f\"Len commit data: {len(config_commit_data)}, {round(len(config_commit_data)/len(commits), 2)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.6f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        \"project_name\": project_name,\n",
    "        \"analysis_time\": elapsed_time,\n",
    "        \"len_commits\": len(commits),\n",
    "        \"config_commit_data\": config_commit_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"/home/simisimon/GitHub/projects/flutter\"\n",
    "\n",
    "commit_data = analyze_repository(repo_path=project_path, get_diff=True)\n",
    "\n",
    "output_file = \"../data/analyzed_projects/freeCodeCamp.json\"\n",
    "\n",
    "print(f\"Write commit data into file {output_file}\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as dest:\n",
    "    json.dump(commit_data, dest, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/simisimon/GitHub/cfgnet_evaluation\"\n",
    "\n",
    "for project_path in glob.glob(project_dir + \"/**\"):\n",
    "    project_name = project_path.split(\"/\")[-1]\n",
    "    \n",
    "    commit_data = analyze_repository(repo_path=project_path, get_diff=True)\n",
    "\n",
    "    output_file = f\"../data/analyzed_projects/{project_name}.json\"\n",
    "\n",
    "    print(f\"Write commit data into file {output_file}\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as dest:\n",
    "        json.dump(commit_data, dest, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract all options and collect all their values across the commit history**\n",
    "\n",
    "Problems (TODO)\n",
    "- options in config files that appear multiple time, such as COPY/ADD/RUN/FROM in Dockerfile\n",
    "- there is no way to reliably track each option seperately\n",
    "- therefore we currently exclude such options\n",
    "\n",
    "Definition of columns\n",
    "- `Changed internally` is an integer, indicating how often the value of an option was changes in the project\n",
    "- `Removed` is a boolean, indicating if an option has been removed at some point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def extract_options(data: List) -> Tuple:\n",
    "    \"\"\"\n",
    "    Extract all options and all of their values from the commit history of a software projects.\n",
    "\n",
    "    :param data: list of configuration data from commit history\n",
    "    :return: tuple of dataframes containing the results and excluded options\n",
    "    \"\"\"\n",
    "\n",
    "    project_name = data[\"project_name\"]\n",
    "    print(f\"Extract all options and their values from {project_name}.\")\n",
    "\n",
    "    # Extract configuration options and their values, excluding duplicates\n",
    "    config_data = []\n",
    "    excluded_pairs = set()\n",
    "    option_presence_tracker = {}  # Track presence across commits\n",
    "\n",
    "    for commit in data[\"config_commit_data\"]:\n",
    "        if commit[\"is_config_related\"]:\n",
    "            commit_hash = commit[\"commit_hash\"]\n",
    "            for file_data in commit[\"network_data\"][\"config_files_data\"]:\n",
    "                # Dictionary to track option occurrences in the current file\n",
    "                option_tracker = {}\n",
    "                for pair in file_data[\"pairs\"]:\n",
    "                    key = (file_data[\"file_path\"], pair[\"option\"])\n",
    "                    \n",
    "                    if key not in option_tracker:\n",
    "                        option_tracker[key] = []\n",
    "                    option_tracker[key].append(pair)\n",
    "                    \n",
    "                    # Update the presence tracker\n",
    "                    if key not in option_presence_tracker:\n",
    "                        option_presence_tracker[key] = {\"last_seen\": commit_hash, \"removed\": False}\n",
    "                    else:\n",
    "                        option_presence_tracker[key][\"last_seen\"] = commit_hash\n",
    "                        option_presence_tracker[key][\"removed\"] = False  # Mark as seen in this commit\n",
    "\n",
    "                # Add only options that appear once in the file\n",
    "                for key, occurrences in option_tracker.items():\n",
    "                    if len(occurrences) == 1:  # Include only unique options\n",
    "                        pair = occurrences[0]\n",
    "                        config_data.append({\n",
    "                            \"file_path\": file_data[\"file_path\"],\n",
    "                            \"option\": pair[\"option\"],\n",
    "                            \"value\": pair[\"value\"],\n",
    "                            \"type\": pair[\"type\"],\n",
    "                            \"concept\": file_data[\"concept\"]\n",
    "                        })\n",
    "                    else:\n",
    "                        pair = occurrences[0]\n",
    "                        excluded_pairs.add((file_data[\"file_path\"], pair[\"option\"], file_data[\"concept\"]))\n",
    "\n",
    "    # After processing all commits, check for removed options\n",
    "    for key, data in option_presence_tracker.items():\n",
    "        if data[\"last_seen\"] != commit_hash:  # If not seen in the last commit, mark as removed\n",
    "            option_presence_tracker[key][\"removed\"] = True\n",
    "\n",
    "    # Create DataFrame from the extracted data\n",
    "    df = pd.DataFrame(config_data)\n",
    "\n",
    "    df_excluded = pd.DataFrame(list(excluded_pairs))\n",
    "\n",
    "    # store excludes options only if dataframe is not empty\n",
    "    if not df_excluded.empty:\n",
    "        df_excluded.columns = [\"File\", \"Option\", \"Concept\"]\n",
    "\n",
    "    # Group by option, type, and file_path, and aggregate unique values\n",
    "    aggregated_df = (\n",
    "        df.groupby(['file_path', 'option', 'concept'])['value']\n",
    "        .apply(lambda x: sorted(list(set(x))))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    aggregated_df.columns = ['File Path', 'Option', 'Concept', 'Values']\n",
    "\n",
    "    # Add and 'changed internally' columns\n",
    "    aggregated_df['Changed internally'] = aggregated_df['Values'].apply(lambda x: len(x) - 1 if len(x) > 1 else 0)\n",
    "\n",
    "    # Add 'removed' column by checking the option presence tracker\n",
    "    removed_status = []\n",
    "    for _, row in aggregated_df.iterrows():\n",
    "        key = (row['File Path'], row['Option'])\n",
    "        removed_status.append(option_presence_tracker.get(key, {}).get('removed', False))\n",
    "\n",
    "    aggregated_df['Removed'] = removed_status\n",
    "\n",
    "    return aggregated_df, df_excluded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "analyzed_project_dir = \"../data/analyzed_projects\"\n",
    "\n",
    "for project_path in glob.glob(analyzed_project_dir + \"/**\"):\n",
    "    with open(project_path, \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "        project_name = data[\"project_name\"]\n",
    "\n",
    "        df_result, df_excluded = extract_options(data=data)\n",
    "\n",
    "        df_excluded.to_csv(f\"../data/excluded_options/{project_name}_excluded.csv\", index=False)\n",
    "        df_result.to_csv(f\"../data/extracted_options/{project_name}_options.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract if an options was set in other projects if the option was changed**\n",
    "\n",
    "Definitions of columns\n",
    "- `Changed globally` is an integer, indicating if an option was changed in other projects\n",
    "- `Set globally` is an integer, indicating the number of projects in which the option exists\n",
    "- `Occurrences globally` is an integer, indicating how often the option occurs across all projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "\n",
    "\n",
    "def analyze_options(target_df, other_dfs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze options in a target file against all other files to compute global stats.\n",
    "\n",
    "    :param target_df: dataframe of target project\n",
    "    :param other_dfs: dataframes of all other projects\n",
    "    :return target_df: updated dataframe of target project\n",
    "    \"\"\"\n",
    "    # Initialize columns\n",
    "    target_df['Set globally'] = 0\n",
    "    target_df['Changed globally'] = 0\n",
    "    target_df['Occurrences globally'] = 0\n",
    "\n",
    "\n",
    "    for index, row in target_df.iterrows():\n",
    "        option = row['Option']\n",
    "\n",
    "        for other_df in other_dfs:\n",
    "            # Find all rows in other_df where the option matches\n",
    "            matching_rows = other_df[other_df['Option'] == option]\n",
    "            match_count = len(matching_rows)\n",
    "\n",
    "            if match_count > 0:\n",
    "                # Increment \"Set in other projects\" by 1 (project-level count)\n",
    "                target_df.loc[index, 'Set globally'] += 1\n",
    "\n",
    "                # Increment \"Total occurrences\" by the total count of matches\n",
    "                target_df.loc[index, 'Occurrences globally'] += match_count\n",
    "\n",
    "                # Check each match for changes in values\n",
    "                for _, match_row in matching_rows.iterrows():\n",
    "                    # Parse the 'Values' column (convert from string to list if necessary)\n",
    "                    raw_values = match_row['Values']\n",
    "                    try:\n",
    "                        values = ast.literal_eval(raw_values) if isinstance(raw_values, str) else raw_values\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        values = [raw_values]  # Fall back to treating as a single value\n",
    "\n",
    "                    # Ensure `values` is iterable\n",
    "                    if not isinstance(values, (list, set, tuple)):\n",
    "                        values = [values]\n",
    "\n",
    "                    unique_values = set(values)\n",
    "                    if len(unique_values) > 1:\n",
    "                        # Increment \"Changed globally\" for each such occurrence\n",
    "                        target_df.loc[index, 'Changed globally'] += 1\n",
    "\n",
    "    return target_df\n",
    "\n",
    "\n",
    "data_dir = \"../data/extracted_options\"\n",
    "\n",
    "# Load all CSV files from the directory into a dictionary of DataFrames\n",
    "repository_files = [file for file in os.listdir(data_dir) if file.endswith('.csv')]\n",
    "repository_dataframes = {file: pd.read_csv(os.path.join(data_dir, file)) for file in repository_files}\n",
    "\n",
    "target_file_name = 'spring-boot-blog_options.csv'\n",
    "target_df = repository_dataframes[target_file_name]\n",
    "\n",
    "# Use all other files as comparison\n",
    "other_dfs = [df for name, df in repository_dataframes.items() if name != target_file_name]\n",
    "\n",
    "# Perform the analysis\n",
    "updated_target_df = analyze_options(target_df.copy(), other_dfs)\n",
    "\n",
    "updated_target_df.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test functions to extract and analyze options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipytest\n",
    "import json\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "def test_extract_options():\n",
    "    \n",
    "    # Load commit history data\n",
    "    with open(\"../data/test_data/projectB_data.json\", \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "\n",
    "    # Extract options\n",
    "    df_results, df_excluded = extract_options(data=data)\n",
    "\n",
    "    # TODO\n",
    "\n",
    "def test_analyze_options():\n",
    "    # Create target_df\n",
    "    target_df = pd.read_csv(\"../data/test_data/projectA_options.csv\")\n",
    "\n",
    "    # Create other_dfs\n",
    "    other_df1 = pd.read_csv(\"../data/test_data/projectB_options.csv\")\n",
    "    other_df2 = pd.read_csv(\"../data/test_data/projectC_options.csv\")\n",
    "    other_dfs = [other_df1, other_df2]\n",
    "\n",
    "    # Analyze configurations\n",
    "    result_df = analyze_options(target_df, other_dfs)\n",
    "\n",
    "\n",
    "    print(result_df.head())\n",
    "\n",
    "    assert result_df.loc[result_df['Option'] == 'EXPOSE', 'Set globally'].iloc[0] == 2\n",
    "    assert result_df.loc[result_df['Option'] == 'EXPOSE', 'Occurrences globally'].iloc[0] == 11\n",
    "    assert result_df.loc[result_df['Option'] == 'EXPOSE', 'Changed globally'].iloc[0] == 3\n",
    "\n",
    "    assert result_df.loc[result_df['Option'] == 'project.version', 'Set globally'].iloc[0] == 2\n",
    "    assert result_df.loc[result_df['Option'] == 'project.version', 'Occurrences globally'].iloc[0] == 11\n",
    "    assert result_df.loc[result_df['Option'] == 'project.version', 'Changed globally'].iloc[0] == 0\n",
    "\n",
    "    assert result_df.loc[result_df['Option'] == 'server.port', 'Set globally'].iloc[0] == 2\n",
    "    assert result_df.loc[result_df['Option'] == 'server.port', 'Occurrences globally'].iloc[0] == 14\n",
    "    assert result_df.loc[result_df['Option'] == 'server.port', 'Changed globally'].iloc[0] == 7\n",
    "\n",
    "ipytest.run(\"-vv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
