{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from cfgnet.network.network_configuration import NetworkConfiguration\n",
    "from cfgnet.network.nodes import ArtifactNode\n",
    "from cfgnet.network.network import Network\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import git\n",
    "import json\n",
    "import subprocess\n",
    "import traceback\n",
    "import glob\n",
    "import time\n",
    "\n",
    "config_file_endings = (\".xml\", \".yml\", \".yaml\", \"Dockerfile\", \".ini\", \".properties\", \".conf\", \".json\", \".toml\", \".cfg\", \"settings.py\", \".cnf\")\n",
    "\n",
    "def checkout_latest_commit(repo, current_branch, latest_commit):\n",
    "     # Return to the latest commit\n",
    "    if current_branch:\n",
    "        # If we were on a branch, return to it\n",
    "        repo.git.checkout(current_branch)\n",
    "        print(f\"Returned to original branch: {current_branch}\")\n",
    "    else:\n",
    "        # If we were in a detached HEAD state, checkout the latest commit directly\n",
    "        repo.git.checkout(latest_commit)\n",
    "        print(f\"Returned to the latest commit: {latest_commit}\")\n",
    "\n",
    "\n",
    "def analyze_config_network(repo_path: str):\n",
    "    \n",
    "    network_config = NetworkConfiguration(\n",
    "        project_root_abs=repo_path,\n",
    "        enable_static_blacklist=False,\n",
    "        enable_internal_links=True,\n",
    "        enable_all_conflicts=True,\n",
    "        enable_file_type_plugins=True,\n",
    "        system_level=False\n",
    "    )\n",
    "\n",
    "    network = Network.init_network(cfg=network_config)\n",
    "\n",
    "    artifacts = network.get_nodes(node_type=ArtifactNode)\n",
    "\n",
    "    config_files_data = []\n",
    "    for artifact in artifacts:\n",
    "        pairs = artifact.get_pairs()\n",
    "\n",
    "        config_files_data.append({\n",
    "            \"file_path\": artifact.rel_file_path,\n",
    "            \"concept\": artifact.concept_name,\n",
    "            \"options\": len(artifact.get_pairs()),\n",
    "            \"pairs\": pairs\n",
    "        })\n",
    "\n",
    "\n",
    "    config_files = set(artifact.rel_file_path for artifact in artifacts)\n",
    "  \t\n",
    "    network_data = {\n",
    "        \"links\": len(network.links),\n",
    "        \"config_files\": list(config_files),\n",
    "        \"config_files_data\": config_files_data\n",
    "    }\n",
    "\n",
    "    return network_data\n",
    "\n",
    "\n",
    "def get_file_diff(repo_path: str, commit, file_path: str):\n",
    "    if commit.parents:\n",
    "        parent_commit = f\"{commit.hexsha}^\"\n",
    "            \n",
    "        try:                        \n",
    "            # Run git diff to capture line-by-line changes\n",
    "            diff_output = subprocess.check_output(\n",
    "                ['git', 'diff', parent_commit, commit.hexsha, '--', file_path],\n",
    "                cwd=repo_path,\n",
    "                text=True\n",
    "            )\n",
    "            return diff_output\n",
    "        except (subprocess.CalledProcessError, git.exc.GitCommandError) as e:\n",
    "            print(f\"Error running git diff for commit {commit.hexsha}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def analyze_repository(repo_path: str, get_diff: bool = False) -> Dict:\n",
    "    \"\"\"Analyze Commit history of repositories and collect stats about the configuration space.\"\"\"  \n",
    "    start_time = time.time()\n",
    "    project_name = repo_path.split(\"/\")[-1]\n",
    "    repo = git.Repo(repo_path)\n",
    "\n",
    "    # Save the current branch to return to it later\n",
    "    current_branch = repo.active_branch.name if not repo.head.is_detached else None\n",
    "    latest_commit = repo.head.commit.hexsha\n",
    "    parent_commit = None\n",
    "\n",
    "    # Get all commits in the repository from oldest to newest\n",
    "    commits = list(repo.iter_commits(\"HEAD\"))[::-1]\n",
    "\n",
    "    print(f\"Number of commits: {len(commits)}\")\n",
    "\n",
    "    config_commit_data = []\n",
    "\n",
    "    for commit in tqdm(commits, desc=\"Processing\", total=len(commits)):\n",
    "\n",
    "        is_config_related = False\n",
    "\n",
    "        # Get commit stats\n",
    "        stats = commit.stats.total\n",
    "\n",
    "        # Checkout the commit\n",
    "        repo.git.checkout(commit.hexsha)\n",
    "\n",
    "        # check if commit is config-related\n",
    "        if any(file_path.endswith(config_file_endings) for file_path in commit.stats.files.keys()):\n",
    "            is_config_related = True\n",
    "            \n",
    "            # Run the external analysis for config-related commits\n",
    "            try: \n",
    "                network_data = analyze_config_network(repo_path=repo_path)\n",
    "            except Exception:\n",
    "                print(f\"Error occurred in commit {commit.hexsha}\")\n",
    "                print({traceback.print_exc()})\n",
    "                return\n",
    "\n",
    "            # Get general stats per config file\n",
    "            for file_path, file_stats in commit.stats.files.items():\n",
    "                \n",
    "                # Get config file data\n",
    "                if file_path in network_data[\"config_files\"]:\n",
    "                    file_data = next(filter(lambda x: x[\"file_path\"] == file_path, network_data[\"config_files_data\"]))\n",
    "                    file_data[\"insertions\"] = file_stats['insertions']\n",
    "                    file_data[\"deletions\"] = file_stats['deletions']\n",
    "                    file_data[\"total_changes\"] = file_stats['insertions'] + file_stats['deletions']\n",
    "\n",
    "                    # Get config file diff\n",
    "                    if get_diff:\n",
    "                        diff_output = get_file_diff(\n",
    "                            repo_path=repo_path,\n",
    "                            commit=commit,\n",
    "                            file_path=file_path\n",
    "                        )\n",
    "\n",
    "                        file_data[\"diff\"] = diff_output\n",
    "\n",
    "            config_commit_data.append(\n",
    "                {   \n",
    "                    \"commit_hash\": str(commit.hexsha),\n",
    "                    \"parent_commit\": (parent_commit),\n",
    "                    \"is_config_related\": is_config_related,\n",
    "                    \"author\": f\"{commit.author.name} <{commit.author.email}>\",\n",
    "                    \"commit_mgs\": str(commit.message),\n",
    "                    \"files_changed\": stats['files'],\n",
    "                    \"insertions\": stats['insertions'],\n",
    "                    \"deletions\": stats['deletions'],\n",
    "                    \"network_data\": network_data\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            config_commit_data.append(\n",
    "                {   \n",
    "                    \"commit_hash\": str(commit.hexsha),\n",
    "                    \"parent_commit\": (parent_commit),\n",
    "                    \"is_config_related\": is_config_related,\n",
    "                    \"author\": f\"{commit.author.name} <{commit.author.email}>\",\n",
    "                    \"commit_mgs\": str(commit.message),\n",
    "                    \"files_changed\": stats['files'],\n",
    "                    \"insertions\": stats['insertions'],\n",
    "                    \"deletions\": stats['deletions'],\n",
    "                    \"network_data\": None\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    # Return to latest commit\n",
    "    checkout_latest_commit(\n",
    "        repo=repo, \n",
    "        current_branch=current_branch,\n",
    "        latest_commit=latest_commit\n",
    "    )\n",
    "\n",
    "    print(f\"Len commit data: {len(config_commit_data)}, {round(len(config_commit_data)/len(commits), 2)}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.6f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        \"project_name\": project_name,\n",
    "        \"analysis_time\": elapsed_time,\n",
    "        \"len_commits\": len(commits),\n",
    "        \"config_commit_data\": config_commit_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/simisimon/GitHub/cfgnet_evaluation\"\n",
    "\n",
    "for project_path in glob.glob(project_dir + \"/**\"):\n",
    "    project_name = project_path.split(\"/\")[-1]\n",
    "    \n",
    "    commit_data = analyze_repository(repo_path=project_path, get_diff=True)\n",
    "\n",
    "    output_file = f\"../data/analyzed_projects/{project_name}.json\"\n",
    "\n",
    "    print(f\"Write commit data into file {output_file}\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as dest:\n",
    "        json.dump(commit_data, dest, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract all options and collect all their values across the commit history**\n",
    "\n",
    "Problems (TODO)\n",
    "- options in config files that appear multiple time, such as COPY/ADD/RUN/FROM in Dockerfile\n",
    "- there is no way to reliably track each option seperately\n",
    "- therefore we currently exclude such options\n",
    "\n",
    "Definition of columns\n",
    "- `Changed internally` is an integer, indicating how often the value of an option was changes in the project\n",
    "- `Removed` is a boolean, indicating if an option has been removed at some point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def extract_options(data: List):\n",
    "\n",
    "    project_name = data[\"project_name\"]\n",
    "    print(f\"Extract all options and their values from {project_name}.\")\n",
    "\n",
    "    # Extract configuration options and their values, excluding duplicates\n",
    "    config_data = []\n",
    "    excluded_pairs = set()\n",
    "    option_presence_tracker = {}  # Track presence across commits\n",
    "\n",
    "    for commit in data[\"config_commit_data\"]:\n",
    "        if commit[\"is_config_related\"]:\n",
    "            commit_hash = commit[\"commit_hash\"]\n",
    "            for file_data in commit[\"network_data\"][\"config_files_data\"]:\n",
    "                # Dictionary to track option occurrences in the current file\n",
    "                option_tracker = {}\n",
    "                for pair in file_data[\"pairs\"]:\n",
    "                    key = (file_data[\"file_path\"], pair[\"option\"])\n",
    "                    \n",
    "                    if key not in option_tracker:\n",
    "                        option_tracker[key] = []\n",
    "                    option_tracker[key].append(pair)\n",
    "                    \n",
    "                    # Update the presence tracker\n",
    "                    if key not in option_presence_tracker:\n",
    "                        option_presence_tracker[key] = {\"last_seen\": commit_hash, \"removed\": False}\n",
    "                    else:\n",
    "                        option_presence_tracker[key][\"last_seen\"] = commit_hash\n",
    "                        option_presence_tracker[key][\"removed\"] = False  # Mark as seen in this commit\n",
    "\n",
    "                # Add only options that appear once in the file\n",
    "                for key, occurrences in option_tracker.items():\n",
    "                    if len(occurrences) == 1:  # Include only unique options\n",
    "                        pair = occurrences[0]\n",
    "                        config_data.append({\n",
    "                            \"file_path\": file_data[\"file_path\"],\n",
    "                            \"option\": pair[\"option\"],\n",
    "                            \"value\": pair[\"value\"],\n",
    "                            \"type\": pair[\"type\"],\n",
    "                            \"concept\": file_data[\"concept\"]\n",
    "                        })\n",
    "                    else:\n",
    "                        pair = occurrences[0]\n",
    "                        excluded_pairs.add((file_data[\"file_path\"], pair[\"option\"], file_data[\"concept\"]))\n",
    "\n",
    "    # After processing all commits, check for removed options\n",
    "    for key, data in option_presence_tracker.items():\n",
    "        if data[\"last_seen\"] != commit_hash:  # If not seen in the last commit, mark as removed\n",
    "            option_presence_tracker[key][\"removed\"] = True\n",
    "\n",
    "    # Create DataFrame from the extracted data\n",
    "    df = pd.DataFrame(config_data)\n",
    "\n",
    "    df_excluded = pd.DataFrame(list(excluded_pairs))\n",
    "\n",
    "    # store excludes options only if dataframe is not empty\n",
    "    if not df_excluded.empty:\n",
    "        df_excluded.columns = [\"File\", \"Option\", \"Concept\"]\n",
    "        df_excluded.to_csv(f\"../data/excluded_options/{project_name}_excluded.csv\", index=False)\n",
    "\n",
    "    # Group by option, type, and file_path, and aggregate unique values\n",
    "    aggregated_df = (\n",
    "        df.groupby(['file_path', 'option', 'concept'])['value']\n",
    "        .apply(lambda x: sorted(list(set(x))))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    aggregated_df.columns = ['File Path', 'Option', 'Concept', 'Values']\n",
    "\n",
    "    # Add and 'changed internally' columns\n",
    "    aggregated_df['Changed internally'] = aggregated_df['Values'].apply(lambda x: len(x) - 1 if len(x) > 1 else 0)\n",
    "\n",
    "    # Add 'removed' column by checking the option presence tracker\n",
    "    removed_status = []\n",
    "    for _, row in aggregated_df.iterrows():\n",
    "        key = (row['File Path'], row['Option'])\n",
    "        removed_status.append(option_presence_tracker.get(key, {}).get('removed', False))\n",
    "\n",
    "    aggregated_df['Removed'] = removed_status\n",
    "\n",
    "    aggregated_df.to_csv(f\"../data/extracted_options/{project_name}_options.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract all options and their values from spring-boot-blog.\n"
     ]
    }
   ],
   "source": [
    "data_file = \"../data/analyzed_projects/spring-boot-blog.json\"\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as src:\n",
    "    data = json.load(src)\n",
    "\n",
    "extract_options(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract all options and their values from mentorship-platform.\n",
      "Extract all options and their values from netflix-oss-example.\n",
      "Extract all options and their values from piggymetrics.\n",
      "Extract all options and their values from spring-boot-blog.\n",
      "Extract all options and their values from taskManagement.\n",
      "Extract all options and their values from test_project_history.\n",
      "Extract all options and their values from Ward.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "analyzed_project_dir = \"../data/analyzed_projects\"\n",
    "\n",
    "for project_path in glob.glob(analyzed_project_dir + \"/**\"):\n",
    "    with open(project_path, \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "\n",
    "    extract_options(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract if an options was set in other projects if the option was changed**\n",
    "\n",
    "Definitions of columns\n",
    "- `Changed globally` is an integer, indicating if an option was changed in other projects\n",
    "- `Set globally` is an integer, indicating the number of projects in which the option exists\n",
    "- `Occurrences globally` is an integer, indicating how often the option occurs across all projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Path</th>\n",
       "      <th>Option</th>\n",
       "      <th>Concept</th>\n",
       "      <th>Values</th>\n",
       "      <th>Changed internally</th>\n",
       "      <th>Removed</th>\n",
       "      <th>Set globally</th>\n",
       "      <th>Changed globally</th>\n",
       "      <th>Occurrences globally</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.mvn/wrapper/maven-wrapper.properties</td>\n",
       "      <td>distributionurl</td>\n",
       "      <td>configparser</td>\n",
       "      <td>['https://repo1.maven.org/maven2/org/apache/ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.mvn/wrapper/maven-wrapper.properties</td>\n",
       "      <td>file</td>\n",
       "      <td>configparser</td>\n",
       "      <td>['.mvn/wrapper/maven-wrapper.properties']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>129</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>docker/Dockerfile</td>\n",
       "      <td>COPY.dest</td>\n",
       "      <td>docker</td>\n",
       "      <td>['$APP_HOME/app.jar']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>docker/Dockerfile</td>\n",
       "      <td>COPY.src</td>\n",
       "      <td>docker</td>\n",
       "      <td>['target/blog-demo-0.0.1-SNAPSHOT.jar']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>docker/Dockerfile</td>\n",
       "      <td>ENTRYPOINT.exec_command</td>\n",
       "      <td>docker</td>\n",
       "      <td>['exec java -jar app.jar']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>src/main/resources/application.properties</td>\n",
       "      <td>spring.h2.console.path</td>\n",
       "      <td>spring</td>\n",
       "      <td>['/h2-console']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>src/main/resources/application.properties</td>\n",
       "      <td>spring.queries.roles-query</td>\n",
       "      <td>spring</td>\n",
       "      <td>['select u.username, r.role from user u inner ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>src/main/resources/application.properties</td>\n",
       "      <td>spring.queries.users-query</td>\n",
       "      <td>spring</td>\n",
       "      <td>['select username, password, active from user ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>src/main/resources/application.properties</td>\n",
       "      <td>spring.thymeleaf.cache</td>\n",
       "      <td>spring</td>\n",
       "      <td>['false']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>src/main/resources/application.properties</td>\n",
       "      <td>spring.thymeleaf.prefix</td>\n",
       "      <td>spring</td>\n",
       "      <td>['classpath:/templates']</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    File Path                      Option  \\\n",
       "0       .mvn/wrapper/maven-wrapper.properties             distributionurl   \n",
       "1       .mvn/wrapper/maven-wrapper.properties                        file   \n",
       "2                           docker/Dockerfile                   COPY.dest   \n",
       "3                           docker/Dockerfile                    COPY.src   \n",
       "4                           docker/Dockerfile     ENTRYPOINT.exec_command   \n",
       "..                                        ...                         ...   \n",
       "69  src/main/resources/application.properties      spring.h2.console.path   \n",
       "70  src/main/resources/application.properties  spring.queries.roles-query   \n",
       "71  src/main/resources/application.properties  spring.queries.users-query   \n",
       "72  src/main/resources/application.properties      spring.thymeleaf.cache   \n",
       "73  src/main/resources/application.properties     spring.thymeleaf.prefix   \n",
       "\n",
       "         Concept                                             Values  \\\n",
       "0   configparser  ['https://repo1.maven.org/maven2/org/apache/ma...   \n",
       "1   configparser          ['.mvn/wrapper/maven-wrapper.properties']   \n",
       "2         docker                              ['$APP_HOME/app.jar']   \n",
       "3         docker            ['target/blog-demo-0.0.1-SNAPSHOT.jar']   \n",
       "4         docker                         ['exec java -jar app.jar']   \n",
       "..           ...                                                ...   \n",
       "69        spring                                    ['/h2-console']   \n",
       "70        spring  ['select u.username, r.role from user u inner ...   \n",
       "71        spring  ['select username, password, active from user ...   \n",
       "72        spring                                          ['false']   \n",
       "73        spring                           ['classpath:/templates']   \n",
       "\n",
       "    Changed internally  Removed  Set globally  Changed globally  \\\n",
       "0                    0    False             3                 2   \n",
       "1                    0    False             6               129   \n",
       "2                    0    False             1                 1   \n",
       "3                    0    False             1                 1   \n",
       "4                    0    False             3                 3   \n",
       "..                 ...      ...           ...               ...   \n",
       "69                   0    False             0                 0   \n",
       "70                   0    False             0                 0   \n",
       "71                   0    False             0                 0   \n",
       "72                   0    False             0                 0   \n",
       "73                   0    False             0                 0   \n",
       "\n",
       "    Occurrences globally  \n",
       "0                      7  \n",
       "1                    136  \n",
       "2                      1  \n",
       "3                      1  \n",
       "4                      3  \n",
       "..                   ...  \n",
       "69                     0  \n",
       "70                     0  \n",
       "71                     0  \n",
       "72                     0  \n",
       "73                     0  \n",
       "\n",
       "[74 rows x 9 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"../data/extracted_options\"\n",
    "\n",
    "# Load all CSV files from the directory into a dictionary of DataFrames\n",
    "repository_files = [file for file in os.listdir(data_dir) if file.endswith('.csv')]\n",
    "repository_dataframes = {file: pd.read_csv(os.path.join(data_dir, file)) for file in repository_files}\n",
    "\n",
    "# Function to analyze options in a target file against all other files\n",
    "def analyze_options_in_target(target_df, other_dfs):\n",
    "    # Initialize columns\n",
    "    target_df['Set globally'] = 0\n",
    "    target_df['Changed globally'] = 0\n",
    "    target_df['Occurrences globally'] = 0\n",
    "\n",
    "\n",
    "    for index, row in target_df.iterrows():\n",
    "        option = row['Option']\n",
    "        values = set()  # To track unique values for \"Changed globally\"\n",
    "\n",
    "        for other_df in other_dfs:\n",
    "            # Count occurrences of the option in the other DataFrame\n",
    "            option_count = other_df['Option'].value_counts().get(option, 0)\n",
    "\n",
    "            # Increment \"Set globally\" (if the option exists)\n",
    "            if option_count > 0:\n",
    "                target_df.loc[index, 'Set globally'] += 1\n",
    "\n",
    "            # Increment \"Occurrences globally\" by the count in this DataFrame\n",
    "            target_df.loc[index, 'Occurrences globally'] += option_count\n",
    "\n",
    "            # Collect unique values for \"Changed globally\"\n",
    "            values.update(other_df.loc[other_df['Option'] == option, 'Values'].explode())\n",
    "\n",
    "        # Set the count of unique values for \"Changed globally\"\n",
    "        target_df.loc[index, 'Changed globally'] = len(values)\n",
    "\n",
    "    return target_df\n",
    "\n",
    "# Pick one target file to analyze (adjust file name as needed)\n",
    "target_file_name = 'spring-boot-blog_options.csv'\n",
    "target_df = repository_dataframes[target_file_name]\n",
    "\n",
    "# Use all other files as comparison\n",
    "other_dfs = [df for name, df in repository_dataframes.items() if name != target_file_name]\n",
    "\n",
    "# Perform the analysis\n",
    "updated_target_df = analyze_options_in_target(target_df.copy(), other_dfs)\n",
    "\n",
    "updated_target_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
