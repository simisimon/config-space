{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crawl Repositories from GitHub using the GitHub API**\n",
    "\n",
    "This notebook crawls the top 10k repositories from GitHub using the GitHub API. \n",
    "As the the GitHub API only return the first 1000 results, we followed a naive approach to avoid this limit by setting ranges of stars to crawl.\n",
    "For each range of stars, we ensured to never hit the first 1000 results. \n",
    "We stopped when we reached 10k repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load env variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# base url\n",
    "BASE_URL = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "# Set up headers with authentication token\n",
    "headers = {'Authorization': f'token {TOKEN}'}\n",
    "\n",
    "# Function to fetch repositories for a given star range\n",
    "def fetch_repositories_by_stars(min_stars, max_stars):\n",
    "    repositories = []\n",
    "    page = 1\n",
    "    per_page = 100  # Maximum allowed per page\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            'q': f'stars:{min_stars}..{max_stars}',\n",
    "            'sort': 'stars',\n",
    "            'order': 'desc',\n",
    "            'per_page': per_page,\n",
    "            'page': page\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, headers=headers, params=params)\n",
    "\n",
    "        # Check for errors\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.json()}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        repos = data.get('items', [])\n",
    "        repositories.extend(repos)\n",
    "\n",
    "        if len(repos) == 0 or len(repositories) >= 10000:\n",
    "            break  # Stop if no more repositories are returned or we hit the 10,000 limit\n",
    "\n",
    "        print(f\"Fetched {len(repositories)} repositories with {min_stars}..{max_stars} stars...\")\n",
    "\n",
    "        page += 1\n",
    "\n",
    "        # Respect GitHub's rate limits by sleeping for a short time\n",
    "        time.sleep(2)\n",
    "\n",
    "    return repositories\n",
    "\n",
    "# Function to fetch the top 10,000 repositories by splitting star ranges\n",
    "def fetch_top_repositories(repo_limit):\n",
    "    repositories = []\n",
    "    star_ranges = [\n",
    "        (400000, 500000), \n",
    "        (300000, 400000), \n",
    "        (200000, 300000), \n",
    "        (100000, 200000), \n",
    "        (90000, 100000),  \n",
    "        (80000, 90000),   \n",
    "        (70000, 80000),   \n",
    "        (60000, 70000),   \n",
    "        (50000, 60000),   \n",
    "        (40000, 50000),   \n",
    "        (30000, 40000),  \n",
    "        (25000, 30000),   \n",
    "        (20000, 25000),\n",
    "        (15000, 20000),\n",
    "        (12500, 15000),   \n",
    "        (12000, 12500),\n",
    "        (11500, 12000),\n",
    "        (11000, 11500),\n",
    "        (10500, 11000),\n",
    "        (10000, 10500),\n",
    "        (9500, 10000), \n",
    "        (9000, 9500),\n",
    "        (8500, 9000),\n",
    "        (8000, 8500),\n",
    "        (7500, 8000),\n",
    "        (7000, 7500),\n",
    "        (6500, 7000),\n",
    "        (6000, 6500),\n",
    "        (5500, 6000),\n",
    "        (5400, 5500),\n",
    "        (5400, 5500),\n",
    "        (5300, 5400),\n",
    "        (5200, 5300),\n",
    "        (5100, 5200),\n",
    "        (5000, 5100),\n",
    "        (4900, 5000),\n",
    "        (4800, 4900),\n",
    "        (4700, 4800),\n",
    "        (4600, 4700),\n",
    "        (4500, 4600),\n",
    "        (4400, 4500),\n",
    "        (4300, 4400),\n",
    "        (4200, 4300),\n",
    "        (4100, 4200),\n",
    "        (4000, 4100),\n",
    "        (3900, 4000),\n",
    "        (3800, 3900),\n",
    "        (3700, 3800),\n",
    "        (3600, 3700),\n",
    "        (3500, 3600),\n",
    "        (3400, 3500),\n",
    "        (3300, 3400),\n",
    "        (3200, 3300),\n",
    "        (3100, 3200),\n",
    "        (3000, 3100),\n",
    "    ]\n",
    "\n",
    "    for min_stars, max_stars in star_ranges:\n",
    "        if len(repositories) >= repo_limit:\n",
    "            break\n",
    "\n",
    "        # Fetch repositories within the star range\n",
    "        repos = fetch_repositories_by_stars(min_stars, max_stars)\n",
    "        repositories.extend(repos)\n",
    "\n",
    "        # Stop once we've hit 10,000 repositories\n",
    "        if len(repositories) >= repo_limit:\n",
    "            break\n",
    "\n",
    "    return repositories[:repo_limit]\n",
    "\n",
    "repo_limit = 2000\n",
    "repositories = fetch_top_repositories(repo_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "ALLOWED_FIELDS = [\n",
    "    \"name\", \"full_name\", \"html_url\", \"description\", \"created_at\", \"updated_at\",\n",
    "    \"size\", \"stargazers_count\", \"language\", \"topics\", \"default_branch\", \"archived\"\n",
    "]\n",
    "\n",
    "filtered_repositories = [\n",
    "    {field: repo.get(field, None) for field in ALLOWED_FIELDS}\n",
    "    for repo in repositories\n",
    "]\n",
    "\n",
    "df_projects = pd.DataFrame(filtered_repositories)\n",
    "\n",
    "df_projects['updated_at'] = pd.to_datetime(df_projects['updated_at'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "three_months_ago = datetime.now() - timedelta(days=30)\n",
    "\n",
    "df_projects['Updated_in_last_30_days'] = df_projects['updated_at'].apply(\n",
    "    lambda x: 'yes' if pd.notnull(x) and x >= three_months_ago else 'no'\n",
    ")\n",
    "\n",
    "df_projects.to_csv(\"../data/projects.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rule-based classifier for real-world status\n",
    "def exclude_repos(row):\n",
    "    name = str(row['name']).lower()\n",
    "    desc = str(row['description']).lower()\n",
    "    size_kb = row['size']\n",
    "\n",
    "    # Exclusion keywords\n",
    "    exclusion_keywords = [\n",
    "        'awesome', 'list', 'tutorial', 'learn', 'book', 'guide', 'exercise',\n",
    "        'course', 'interview', 'template', 'starter', 'demo', 'example',\n",
    "        'algorithm', 'data-structure', \"cheatsheet\"\n",
    "    ]\n",
    "\n",
    "    # Exclude if keyword is in name, description, or topics\n",
    "    for keyword in exclusion_keywords:\n",
    "        if keyword in name or keyword in desc:\n",
    "            return 'n'\n",
    "\n",
    "    # Positive signal: substantial codebase size\n",
    "    if size_kb >= 1000:\n",
    "        return 'y'\n",
    "\n",
    "    return 'n'\n",
    "\n",
    "# Apply real-world classification\n",
    "df_projects['relevance'] = df_projects.apply(exclude_repos, axis=1)\n",
    "\n",
    "df_projects.to_csv(\"../data/projects.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the project CSV file\n",
    "df = pd.read_csv(\"projects.csv\")\n",
    "\n",
    "# Convert 'updated_at' to datetime and make timezone-naive\n",
    "df['updated_at'] = pd.to_datetime(df['updated_at'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "# Calculate threshold for last 3 months\n",
    "three_months_ago = datetime.now() - timedelta(days=90)\n",
    "\n",
    "# Relevance annotation: recently updated = relevant\n",
    "df['relevance'] = df['updated_at'].apply(\n",
    "    lambda x: 'relevant' if pd.notnull(x) and x >= three_months_ago else 'not relevant'\n",
    ")\n",
    "\n",
    "# Define rule-based classifier for real-world status\n",
    "def classify_real_world_project(row):\n",
    "    name = str(row['name']).lower()\n",
    "    desc = str(row['description']).lower()\n",
    "    try:\n",
    "        topics = [t.lower() for t in eval(row['topics'])] if pd.notnull(row['topics']) else []\n",
    "    except Exception:\n",
    "        topics = []\n",
    "    size_kb = row['size']\n",
    "\n",
    "    # Exclusion keywords\n",
    "    exclusion_keywords = [\n",
    "        'awesome', 'list', 'tutorial', 'learn', 'book', 'guide', 'exercise',\n",
    "        'course', 'interview', 'template', 'starter', 'demo', 'example',\n",
    "        'algorithm', 'data-structure'\n",
    "    ]\n",
    "\n",
    "    # Exclude if keyword is in name, description, or topics\n",
    "    for keyword in exclusion_keywords:\n",
    "        if keyword in name or keyword in desc or any(keyword in topic for topic in topics):\n",
    "            return 'not real-world'\n",
    "\n",
    "    # Positive signal: substantial codebase size\n",
    "    if size_kb >= 1000:\n",
    "        return 'real-world'\n",
    "\n",
    "    return 'uncertain'\n",
    "\n",
    "# Apply real-world classification\n",
    "df['real_world_status'] = df.apply(classify_real_world_project, axis=1)\n",
    "\n",
    "# Save annotated file\n",
    "df.to_csv(\"projects_annotated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the large JSON file\n",
    "with open('../data/projects_raw.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Calculate split size\n",
    "num_splits = 10\n",
    "split_size = len(data) // num_splits\n",
    "\n",
    "# Write each split to a new JSON file\n",
    "for i in range(num_splits):\n",
    "    start = i * split_size\n",
    "    # Ensure last split gets remaining items\n",
    "    end = (i + 1) * split_size if i < num_splits - 1 else len(data)\n",
    "    split_data = data[start:end]\n",
    "    \n",
    "    with open(f'../data/projects_raw_{i+1}.json', 'w') as f:\n",
    "        json.dump(split_data, f, indent=2)\n",
    "\n",
    "print(\"JSON file successfully split into 5 smaller files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crawl Repositories from GitHub using the GitHub API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time\n",
    "\n",
    "# load env variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "GITHUB_API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\n",
    "GITHUB_API_URL = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"token {GITHUB_API_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "MAX_RESULTS = 300  # Stop after finding 100 matching repositories\n",
    "PER_PAGE = 30  # Number of repos per page (max 100)\n",
    "MATCHING_REPOS = []  # Store valid repositories\n",
    "\n",
    "\n",
    "def search_repositories(page):\n",
    "    \"\"\"Search for Java repositories mentioning Spring Boot (paginated)\"\"\"\n",
    "    search_url = \"https://api.github.com/search/repositories\"\n",
    "    query = 'spring-boot language:Java'\n",
    "\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"sort\": \"stars\",\n",
    "        \"order\": \"desc\",\n",
    "        \"per_page\": PER_PAGE,\n",
    "        \"page\": page\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, headers=HEADERS, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"items\", [])\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, Message: {response.json()}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_repositories(repositories):\n",
    "    \"\"\"Check repositories for Maven, Docker, and Docker Compose files\"\"\"\n",
    "    global MATCHING_REPOS\n",
    "\n",
    "    for repo in repositories:\n",
    "        if len(MATCHING_REPOS) >= MAX_RESULTS:\n",
    "            return  # Stop if we already found enough\n",
    "\n",
    "        repo_name = repo[\"full_name\"]\n",
    "        default_branch = repo[\"default_branch\"]\n",
    "\n",
    "        file_tree = get_repo_file_tree(repo_name, default_branch)\n",
    "\n",
    "        # Check if required files exist anywhere in the repo\n",
    "        has_maven = any(\"pom.xml\" in file for file in file_tree)\n",
    "        has_spring = any(\"application.properties\" in file for file in file_tree) or any(\"application.yml\" in file for file in file_tree)\n",
    "        has_docker = any(\"Dockerfile\" in file for file in file_tree)\n",
    "        has_compose = any(\"docker-compose.yml\" in file for file in file_tree)\n",
    "\n",
    "        if has_maven and has_spring and (has_docker or has_compose):\n",
    "\n",
    "            print(f\"⭐ {repo['stargazers_count']} | {repo_name} | {repo['html_url']}\")\n",
    "            MATCHING_REPOS.append(repo)\n",
    "        else:\n",
    "            print(f\"❌ {repo_name} does not have the required files.\")\n",
    "\n",
    "\n",
    "\n",
    "def get_repo_file_tree(repo_full_name, default_branch):\n",
    "    \"\"\"Retrieve the full file tree of a repository\"\"\"\n",
    "    tree_url = f\"https://api.github.com/repos/{repo_full_name}/git/trees/{default_branch}?recursive=1\"\n",
    "    \n",
    "    response = requests.get(tree_url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        return [file[\"path\"] for file in response.json().get(\"tree\", [])]\n",
    "    return []\n",
    "\n",
    "\n",
    "page = 1\n",
    "while len(MATCHING_REPOS) < MAX_RESULTS:\n",
    "    print(f\"Fetching page {page}...\")\n",
    "\n",
    "    repositories = search_repositories(page)\n",
    "    if not repositories:\n",
    "        break  # Stop if no more results\n",
    "\n",
    "    process_repositories(repositories)\n",
    "\n",
    "    page += 1\n",
    "    time.sleep(2)  # Add a delay to avoid hitting rate limits\n",
    "\n",
    "# Save the matching repositories to a JSON file\n",
    "with open(f\"../data/micro_service_projects.json\", \"w\") as dest:\n",
    "    json.dump(MATCHING_REPOS, dest, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Found {len(MATCHING_REPOS)} repositories matching all conditions.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"../data/microservice_projects.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Normalize and filter specific fields\n",
    "df = pd.json_normalize(data)[[\"stargazers_count\", \"name\", \"full_name\", \"description\", \"html_url\"]]\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.rename(columns={\n",
    "    \"stargazers_count\": \"stars\",\n",
    "    \"html_url\": \"url\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"../data/microservice_projects.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "files = [file_name for file_name in glob.glob(\"../data/microservice_projects/*.json\")]\n",
    "\n",
    "len(files)\n",
    "\n",
    "\n",
    "with open(\"../slurm/test.txt\", \"w\", encoding=\"utf-8\") as dest:\n",
    "    for file_name in files:\n",
    "        try:\n",
    "            with open(file_name, \"r\", encoding=\"utf-8\") as src:\n",
    "                repo = json.load(src)\n",
    "                dest.write(f\"python3 analyze.py --url={repo['html_url']} --name={repo['name']}\" + \"\\n\")\n",
    "        except:\n",
    "            print(\"Skipping file\", file_name)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crawl top 200 Repositories from GitHub using the GitHub API**\n",
    "\n",
    "- last scraped at: 22.04.2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# GitHub API token and base URL\n",
    "GITHUB_API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\n",
    "BASE_URL = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "# Headers for authentication\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"token {GITHUB_API_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to fetch repositories\n",
    "def fetch_top_repositories(limit=200):\n",
    "    repositories = []\n",
    "    page = 1\n",
    "    per_page = 100  # Maximum allowed per page\n",
    "\n",
    "    while len(repositories) < limit:\n",
    "        params = {\n",
    "            \"q\": \"stars:>0\",\n",
    "            \"sort\": \"stars\",\n",
    "            \"order\": \"desc\",\n",
    "            \"per_page\": per_page,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.json()}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        repos = data.get(\"items\", [])\n",
    "        repositories.extend(repos)\n",
    "\n",
    "        if len(repos) == 0:\n",
    "            break  # Stop if no more repositories are returned\n",
    "\n",
    "        print(f\"Fetched {len(repositories)} repositories so far...\")\n",
    "        page += 1\n",
    "\n",
    "        # Respect GitHub's rate limits\n",
    "        time.sleep(2)\n",
    "\n",
    "    return repositories[:limit]\n",
    "\n",
    "# Fetch the top 200 repositories\n",
    "top_repositories = fetch_top_repositories(limit=200)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_csv_file = \"../data/popularity_projects.csv\"\n",
    "os.makedirs(os.path.dirname(output_csv_file), exist_ok=True)\n",
    "\n",
    "with open(output_csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    # Write the header\n",
    "    csvwriter.writerow([\"stars\", \"name\", \"full_name\", \"html_url\", \"description\"])\n",
    "    # Write the repository data\n",
    "    for repo in top_repositories:\n",
    "        csvwriter.writerow([repo.get(\"stargazers_count\"), repo.get(\"name\"), repo.get(\"full_name\"), repo.get(\"html_url\"), repo.get(\"description\")])\n",
    "\n",
    "print(f\"✅ Successfully saved {len(top_repositories)} repositories to {output_csv_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# GitHub API token and base URL\n",
    "GITHUB_API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\n",
    "BASE_URL = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "# Headers for authentication\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"token {GITHUB_API_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Keywords to exclude toy projects, courses, and tutorials\n",
    "EXCLUDE_KEYWORDS = [\"tutorial\", \"course\", \"example\", \"demo\", \"test\", \"sample\"]\n",
    "\n",
    "# Function to fetch repositories for a given star range\n",
    "def fetch_repositories_by_stars(min_stars, max_stars, limit=100):\n",
    "    repositories = []\n",
    "    page = 1\n",
    "    per_page = 30  # Maximum allowed per page\n",
    "\n",
    "    while len(repositories) < limit:\n",
    "        params = {\n",
    "            \"q\": f\"stars:{min_stars}..{max_stars}\",\n",
    "            \"sort\": \"stars\",\n",
    "            \"order\": \"desc\",\n",
    "            \"per_page\": per_page,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.json()}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        repos = data.get(\"items\", [])\n",
    "\n",
    "        # Filter out toy projects, courses, and tutorials\n",
    "        for repo in repos:\n",
    "            if len(repositories) >= limit:\n",
    "                break\n",
    "\n",
    "            repositories.append(repo)\n",
    "\n",
    "        if len(repos) == 0:\n",
    "            break  # Stop if no more repositories are returned\n",
    "\n",
    "        print(f\"Fetched {len(repositories)} repositories so far...\")\n",
    "        page += 1\n",
    "\n",
    "        # Respect GitHub's rate limits\n",
    "        time.sleep(2)\n",
    "\n",
    "    return repositories[:limit]\n",
    "\n",
    "# Fetch top 100 most popular, medium popular, and low popular projects\n",
    "top_projects = fetch_repositories_by_stars(50000, 500000, limit=250)\n",
    "medium_projects = fetch_repositories_by_stars(5000, 50000, limit=250)\n",
    "low_projects = fetch_repositories_by_stars(0, 5000, limit=250)\n",
    "\n",
    "# Save the results to CSV files\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "def save_to_csv(filename, projects):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        # Write the header\n",
    "        csvwriter.writerow([\"stars\", \"name\", \"full_name\", \"html_url\", \"description\"])\n",
    "        # Write the repository data\n",
    "        for repo in projects:\n",
    "            csvwriter.writerow([\n",
    "                repo.get(\"stargazers_count\"),\n",
    "                repo.get(\"name\"),\n",
    "                repo.get(\"full_name\"),\n",
    "                repo.get(\"html_url\"),\n",
    "                repo.get(\"description\")\n",
    "            ])\n",
    "\n",
    "save_to_csv(\"../data/top_projects.csv\", top_projects)\n",
    "save_to_csv(\"../data/medium_projects.csv\", medium_projects)\n",
    "save_to_csv(\"../data/low_projects.csv\", low_projects)\n",
    "\n",
    "print(\"✅ Successfully saved projects to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
