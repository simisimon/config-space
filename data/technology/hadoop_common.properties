hadoop.common.configuration.version=0.23.0
hadoop.tmp.dir=/tmp/hadoop-${user.name}
io.native.lib.available=true
hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter
hadoop.security.authorization=false
hadoop.security.instrumentation.requires.admin=false
hadoop.security.authentication=simple
hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
hadoop.security.dns.interface=
hadoop.security.dns.nameserver=
hadoop.security.dns.log-slow-lookups.enabled=false
hadoop.security.dns.log-slow-lookups.threshold.ms=1000
hadoop.security.groups.cache.secs=300
hadoop.security.groups.negative-cache.secs=30
hadoop.security.groups.cache.warn.after.ms=5000
hadoop.security.groups.cache.background.reload=false
hadoop.security.groups.cache.background.reload.threads=3
hadoop.security.group.mapping.ldap.connection.timeout.ms=60000
hadoop.security.group.mapping.ldap.read.timeout.ms=60000
hadoop.security.group.mapping.ldap.url=
hadoop.security.group.mapping.ldap.ssl=false
hadoop.security.group.mapping.ldap.ssl.keystore=
hadoop.security.group.mapping.ldap.ssl.keystore.password.file=
hadoop.security.group.mapping.ldap.bind.user=
hadoop.security.group.mapping.ldap.bind.password.file=
hadoop.security.group.mapping.ldap.base=
hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
hadoop.security.group.mapping.ldap.search.attr.member=member
hadoop.security.group.mapping.ldap.search.attr.group.name=cn
hadoop.security.group.mapping.ldap.posix.attr.uid.name=uidNumber
hadoop.security.group.mapping.ldap.posix.attr.gid.name=gidNumber
hadoop.security.group.mapping.ldap.directory.search.timeout=10000
hadoop.security.group.mapping.providers=
hadoop.security.group.mapping.providers.combined=true
hadoop.security.service.user.name.key=
hadoop.security.uid.cache.secs=14400
hadoop.rpc.protection=authentication
hadoop.security.saslproperties.resolver.class=
hadoop.security.sensitive-config-keys=secret$,password$,ssl.keystore.pass$,fs.s3.*[Ss]ecret.?[Kk]ey,fs.azure.account.key.*,dfs.webhdfs.oauth2.[a-z]+.token,hadoop.security.sensitive-config-keys
hadoop.workaround.non.threadsafe.getpwuid=true
hadoop.kerberos.kinit.command=kinit
hadoop.kerberos.min.seconds.before.relogin=60
hadoop.security.auth_to_local=
io.file.buffer.size=4096
io.bytes.per.checksum=512
io.skip.checksum.errors=false
io.compression.codecs=
io.compression.codec.bzip2.library=system-native
io.serializations=org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
io.seqfile.local.dir=${hadoop.tmp.dir}/io/local
io.map.index.skip=0
io.map.index.interval=128
fs.defaultFS=file:///
fs.default.name=file:///
fs.trash.interval=0
fs.trash.checkpoint.interval=0
fs.protected.directories=
fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs
fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs
fs.AbstractFileSystem.webhdfs.impl=org.apache.hadoop.fs.WebHdfs
fs.AbstractFileSystem.swebhdfs.impl=org.apache.hadoop.fs.SWebHdfs
fs.ftp.host=0.0.0.0
fs.ftp.host.port=21
fs.df.interval=60000
fs.du.interval=600000
fs.s3.block.size=67108864
fs.s3.buffer.dir=${hadoop.tmp.dir}/s3
fs.s3.maxRetries=4
fs.s3.sleepTimeSeconds=10
fs.automatic.close=true
fs.s3n.block.size=67108864
fs.s3n.multipart.uploads.enabled=false
fs.s3n.multipart.uploads.block.size=67108864
fs.s3n.multipart.copy.block.size=5368709120
fs.s3n.server-side-encryption-algorithm=
fs.s3a.access.key=
fs.s3a.secret.key=
fs.s3a.aws.credentials.provider=
fs.s3a.session.token=
fs.s3a.security.credential.provider.path=
fs.s3a.connection.maximum=15
fs.s3a.connection.ssl.enabled=true
fs.s3a.endpoint=
fs.s3a.path.style.access=false
fs.s3a.proxy.host=
fs.s3a.proxy.port=
fs.s3a.proxy.username=
fs.s3a.proxy.password=
fs.s3a.proxy.domain=
fs.s3a.proxy.workstation=
fs.s3a.attempts.maximum=20
fs.s3a.connection.establish.timeout=5000
fs.s3a.connection.timeout=200000
fs.s3a.socket.send.buffer=8192
fs.s3a.socket.recv.buffer=8192
fs.s3a.paging.maximum=5000
fs.s3a.threads.max=10
fs.s3a.threads.keepalivetime=60
fs.s3a.max.total.tasks=5
fs.s3a.multipart.size=100M
fs.s3a.multipart.threshold=2147483647
fs.s3a.multiobjectdelete.enable=true
fs.s3a.acl.default=
fs.s3a.multipart.purge=false
fs.s3a.multipart.purge.age=86400
fs.s3a.server-side-encryption-algorithm=
fs.s3a.signing-algorithm=
fs.s3a.block.size=32M
fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a
fs.s3a.fast.upload=false
fs.s3a.fast.upload.buffer=disk
fs.s3a.fast.upload.active.blocks=4
fs.s3a.readahead.range=64K
fs.s3a.user.agent.prefix=
fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A
io.seqfile.compress.blocksize=1000000
io.mapfile.bloom.size=1048576
io.mapfile.bloom.error.rate=0.005
hadoop.util.hash.type=murmur
ipc.client.idlethreshold=4000
ipc.client.kill.max=10
ipc.client.connection.maxidletime=10000
ipc.client.connect.max.retries=10
ipc.client.connect.retry.interval=1000
ipc.client.connect.timeout=20000
ipc.client.connect.max.retries.on.timeouts=45
ipc.client.tcpnodelay=true
ipc.client.low-latency=false
ipc.client.ping=true
ipc.ping.interval=60000
ipc.client.rpc-timeout.ms=0
ipc.server.listen.queue.size=128
ipc.server.log.slow.rpc=false
ipc.maximum.data.length=67108864
ipc.maximum.response.length=134217728
hadoop.security.impersonation.provider.class=
hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
hadoop.rpc.socket.factory.class.ClientProtocol=
hadoop.socks.server=
net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
net.topology.impl=org.apache.hadoop.net.NetworkTopology
net.topology.script.file.name=
net.topology.script.number.args=100
net.topology.table.file.name=
file.stream-buffer-size=4096
file.bytes-per-checksum=512
file.client-write-packet-size=65536
file.blocksize=67108864
file.replication=1
s3.stream-buffer-size=4096
s3.bytes-per-checksum=512
s3.client-write-packet-size=65536
s3.blocksize=67108864
s3.replication=3
s3native.stream-buffer-size=4096
s3native.bytes-per-checksum=512
s3native.client-write-packet-size=65536
s3native.blocksize=67108864
s3native.replication=3
ftp.stream-buffer-size=4096
ftp.bytes-per-checksum=512
ftp.client-write-packet-size=65536
ftp.blocksize=67108864
ftp.replication=3
tfile.io.chunk.size=1048576
tfile.fs.output.buffer.size=262144
tfile.fs.input.buffer.size=262144
hadoop.http.authentication.type=simple
hadoop.http.authentication.token.validity=36000
hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret
hadoop.http.authentication.cookie.domain=
hadoop.http.authentication.simple.anonymous.allowed=true
hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab
hadoop.http.cross-origin.enabled=false
hadoop.http.cross-origin.allowed-origins=*
hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD
hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin
hadoop.http.cross-origin.max-age=1800
dfs.ha.fencing.methods=
dfs.ha.fencing.ssh.connect-timeout=30000
dfs.ha.fencing.ssh.private-key-files=
hadoop.http.staticuser.user=dr.who
ha.zookeeper.quorum=
ha.zookeeper.session-timeout.ms=5000
ha.zookeeper.parent-znode=/hadoop-ha
ha.zookeeper.acl=world:anyone:rwcda
ha.zookeeper.auth=
hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
hadoop.ssl.require.client.cert=false
hadoop.ssl.hostname.verifier=DEFAULT
hadoop.ssl.server.conf=ssl-server.xml
hadoop.ssl.client.conf=ssl-client.xml
hadoop.ssl.enabled=false
hadoop.ssl.enabled.protocols=TLSv1
hadoop.jetty.logs.serve.aliases=true
fs.permissions.umask-mode=022
ha.health-monitor.connect-retry-interval.ms=1000
ha.health-monitor.check-interval.ms=1000
ha.health-monitor.sleep-after-disconnect.ms=1000
ha.health-monitor.rpc-timeout.ms=45000
ha.failover-controller.new-active.rpc-timeout.ms=60000
ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
ha.failover-controller.graceful-fence.connection.retries=1
ha.failover-controller.cli-check.rpc-timeout.ms=20000
ipc.client.fallback-to-simple-auth-allowed=false
fs.client.resolve.remote.symlinks=true
nfs.exports.allowed.hosts=* rw
hadoop.user.group.static.mapping.overrides=dr.who=;
rpc.metrics.quantile.enable=false
rpc.metrics.percentiles.intervals=
hadoop.security.crypto.codec.classes.EXAMPLECIPHERSUITE=
hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec
hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding
hadoop.security.crypto.jce.provider=
hadoop.security.crypto.buffer.size=8192
hadoop.security.java.secure.random.algorithm=SHA1PRNG
hadoop.security.secure.random.impl=
hadoop.security.random.device.file.path=/dev/urandom
hadoop.security.key.provider.path=
fs.har.impl.disable.cache=true
hadoop.security.kms.client.authentication.retry-count=1
hadoop.security.kms.client.encrypted.key.cache.size=500
hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2
hadoop.security.kms.client.encrypted.key.cache.expiry=43200000
ipc.server.max.connections=0
hadoop.registry.rm.enabled=false
hadoop.registry.zk.root=/registry
hadoop.registry.zk.session.timeout.ms=60000
hadoop.registry.zk.connection.timeout.ms=15000
hadoop.registry.zk.retry.times=5
hadoop.registry.zk.retry.interval.ms=1000
hadoop.registry.zk.retry.ceiling.ms=60000
hadoop.registry.zk.quorum=localhost:2181
hadoop.registry.secure=false
hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@
hadoop.registry.kerberos.realm=
hadoop.registry.jaas.context=Client
hadoop.shell.missing.defaultFs.warning=false
hadoop.shell.safely.delete.limit.num.files=100
fs.client.htrace.sampler.classes=
hadoop.htrace.span.receiver.classes=
hadoop.caller.context.enabled=false
hadoop.caller.context.max.size=128
hadoop.caller.context.signature.max.size=40
