hadoop.hdfs.configuration.version=1
dfs.namenode.rpc-address=
dfs.namenode.rpc-bind-host=
dfs.namenode.servicerpc-address=
dfs.namenode.servicerpc-bind-host=
dfs.namenode.lifeline.rpc-address=
dfs.namenode.lifeline.rpc-bind-host=
dfs.namenode.secondary.http-address=0.0.0.0:9868
dfs.namenode.secondary.https-address=0.0.0.0:9869
dfs.datanode.address=0.0.0.0:9866
dfs.datanode.http.address=0.0.0.0:9864
dfs.datanode.ipc.address=0.0.0.0:9867
dfs.datanode.http.internal-proxy.port=0
dfs.datanode.handler.count=10
dfs.namenode.http-address=0.0.0.0:9870
dfs.namenode.http-bind-host=
dfs.namenode.heartbeat.recheck-interval=300000
dfs.http.policy=HTTP_ONLY
dfs.client.https.need-auth=false
dfs.client.cached.conn.retry=3
dfs.https.server.keystore.resource=ssl-server.xml
dfs.client.https.keystore.resource=ssl-client.xml
dfs.datanode.https.address=0.0.0.0:9865
dfs.namenode.https-address=0.0.0.0:9871
dfs.namenode.https-bind-host=
dfs.datanode.dns.interface=default
dfs.datanode.dns.nameserver=default
dfs.namenode.backup.address=0.0.0.0:50100
dfs.namenode.backup.http-address=0.0.0.0:50105
dfs.namenode.redundancy.considerLoad=true
dfs.namenode.redundancy.considerLoadByStorageType=false
dfs.namenode.redundancy.considerLoad.factor=2.0
dfs.namenode.redundancy.considerLoadByVolume=false
dfs.namenode.read.considerLoad=false
dfs.namenode.read.considerStorageType=false
dfs.datanode.httpserver.filter.handlers=org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler
dfs.default.chunk.view.size=32768
dfs.datanode.du.reserved.calculator=org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAbsolute
dfs.datanode.du.reserved=0
dfs.datanode.du.reserved.pct=0
dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/name
dfs.namenode.name.dir.restore=false
dfs.namenode.fs-limits.max-component-length=255
dfs.namenode.fs-limits.max-directory-items=1048576
dfs.namenode.fs-limits.min-block-size=1048576
dfs.namenode.fs-limits.max-blocks-per-file=10000
dfs.namenode.edits.dir=${dfs.namenode.name.dir}
dfs.namenode.edits.dir.required=
dfs.namenode.shared.edits.dir=
dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager
dfs.namenode.edits.qjournals.resolution-enabled=false
dfs.namenode.edits.qjournals.resolver.impl=
dfs.permissions.enabled=true
dfs.permissions.ContentSummary.subAccess=false
dfs.permissions.superusergroup=supergroup
dfs.cluster.administrators=
dfs.namenode.ip-proxy-users=
dfs.namenode.acls.enabled=true
dfs.namenode.posix.acl.inheritance.enabled=true
dfs.namenode.lazypersist.file.scrub.interval.sec=300
dfs.block.access.token.enable=false
dfs.block.access.key.update.interval=600
dfs.block.access.token.lifetime=600
dfs.block.access.token.protobuf.enable=false
dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data
dfs.datanode.data.dir.perm=700
dfs.replication=3
dfs.replication.max=512
dfs.namenode.replication.min=1
dfs.namenode.maintenance.replication.min=1
dfs.namenode.safemode.replication.min=
dfs.namenode.max-corrupt-file-blocks-returned=100
dfs.blocksize=134217728
dfs.client.block.write.retries=3
dfs.client.block.write.replace-datanode-on-failure.enable=true
dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT
dfs.client.block.write.replace-datanode-on-failure.best-effort=false
dfs.client.block.write.replace-datanode-on-failure.min-replication=0
dfs.blockreport.intervalMsec=21600000
dfs.blockreport.initialDelay=0
dfs.blockreport.split.threshold=1000000
dfs.namenode.max.full.block.report.leases=6
dfs.namenode.full.block.report.lease.length.ms=300000
dfs.datanode.directoryscan.interval=21600
dfs.datanode.directoryscan.threads=1
dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000
dfs.datanode.reconcile.blocks.batch.size=1000
dfs.datanode.reconcile.blocks.batch.interval=2000
dfs.heartbeat.interval=3
dfs.datanode.lifeline.interval.seconds=
dfs.namenode.handler.count=10
dfs.namenode.service.handler.count=10
dfs.namenode.lifeline.handler.ratio=0.10
dfs.namenode.lifeline.handler.count=
dfs.namenode.safemode.threshold-pct=0.999f
dfs.namenode.safemode.min.datanodes=0
dfs.namenode.safemode.recheck.interval=1000
dfs.namenode.safemode.extension=30000
dfs.namenode.resource.check.interval=5000
dfs.namenode.resource.du.reserved=104857600
dfs.namenode.resource.checked.volumes=
dfs.namenode.resource.checked.volumes.minimum=1
dfs.datanode.balance.bandwidthPerSec=100m
dfs.hosts=
dfs.hosts.exclude=
dfs.namenode.max.objects=0
dfs.namenode.datanode.registration.ip-hostname-check=true
dfs.namenode.decommission.interval=30
dfs.namenode.decommission.blocks.per.interval=500000
dfs.namenode.decommission.max.concurrent.tracked.nodes=100
dfs.namenode.decommission.monitor.class=org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor
dfs.namenode.decommission.backoff.monitor.pending.limit=10000
dfs.namenode.decommission.backoff.monitor.pending.blocks.per.lock=1000
dfs.namenode.redundancy.interval.seconds=3
dfs.namenode.redundancy.queue.restart.iterations=2400
dfs.namenode.accesstime.precision=3600000
dfs.datanode.plugins=
dfs.namenode.plugins=
dfs.namenode.block-placement-policy.default.prefer-local-node=true
dfs.stream-buffer-size=4096
dfs.bytes-per-checksum=512
dfs.client-write-packet-size=65536
dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000
dfs.client.write.recover.lease.on.close.exception=false
dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary
dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}
dfs.namenode.checkpoint.period=3600
dfs.namenode.checkpoint.txns=1000000
dfs.namenode.checkpoint.check.period=60
dfs.namenode.checkpoint.max-retries=3
dfs.namenode.checkpoint.check.quiet-multiplier=1.5
dfs.namenode.num.checkpoints.retained=2
dfs.namenode.num.extra.edits.retained=1000000
dfs.namenode.max.extra.edits.segments.retained=10000
dfs.namenode.delegation.key.update-interval=86400000
dfs.namenode.delegation.token.max-lifetime=604800000
dfs.namenode.delegation.token.renew-interval=86400000
dfs.datanode.failed.volumes.tolerated=0
dfs.datanode.volumes.replica-add.threadpool.size=
dfs.image.compress=false
dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
dfs.image.transfer.timeout=60000
dfs.image.transfer.bandwidthPerSec=52428800
dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0
dfs.image.transfer.chunksize=65536
dfs.image.parallel.load=false
dfs.image.parallel.target.sections=12
dfs.image.parallel.inode.threshold=1000000
dfs.image.parallel.threads=4
dfs.edit.log.transfer.timeout=30000
dfs.edit.log.transfer.bandwidthPerSec=0
dfs.namenode.support.allow.format=true
dfs.datanode.max.transfer.threads=4096
dfs.datanode.scan.period.hours=504
dfs.block.scanner.volume.bytes.per.second=1048576
dfs.block.scanner.skip.recent.accessed=false
dfs.block.scanner.volume.join.timeout.ms=5000
dfs.datanode.readahead.bytes=4194304
dfs.datanode.drop.cache.behind.reads=false
dfs.datanode.drop.cache.behind.writes=false
dfs.datanode.sync.behind.writes=false
dfs.client.failover.max.attempts=15
dfs.client.failover.sleep.base.millis=500
dfs.client.failover.sleep.max.millis=15000
dfs.client.failover.connection.retries=0
dfs.client.failover.connection.retries.on.timeouts=0
dfs.client.datanode-restart.timeout=30
dfs.nameservices=
dfs.nameservice.id=
dfs.internal.nameservices=
dfs.ha.namenodes.EXAMPLENAMESERVICE=
dfs.ha.namenode.id=
dfs.ha.log-roll.period=120
dfs.ha.tail-edits.period=60
dfs.ha.tail-edits.period.backoff-max=0
dfs.ha.tail-edits.namenode-retries=3
dfs.ha.tail-edits.rolledits.timeout=60
dfs.ha.automatic-failover.enabled=false
dfs.client.use.datanode.hostname=false
dfs.datanode.use.datanode.hostname=false
dfs.client.local.interfaces=
dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp
dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000
dfs.namenode.kerberos.principal=
dfs.namenode.keytab.file=
dfs.datanode.kerberos.principal=
dfs.datanode.keytab.file=
dfs.journalnode.kerberos.principal=
dfs.journalnode.keytab.file=
dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
dfs.journalnode.kerberos.internal.spnego.principal=
dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
dfs.web.authentication.kerberos.principal=
dfs.web.authentication.kerberos.keytab=
dfs.namenode.kerberos.principal.pattern=*
dfs.namenode.avoid.read.stale.datanode=false
dfs.namenode.avoid.read.slow.datanode=false
dfs.namenode.avoid.write.stale.datanode=false
dfs.namenode.enable.log.stale.datanode=false
dfs.namenode.stale.datanode.interval=30000
dfs.namenode.write.stale.datanode.ratio=0.5f
dfs.namenode.invalidate.work.pct.per.iteration=0.32f
dfs.namenode.replication.work.multiplier.per.iteration=2
nfs.server.port=2049
nfs.mountd.port=4242
nfs.dump.dir=/tmp/.hdfs-nfs
nfs.rtmax=1048576
nfs.wtmax=1048576
nfs.keytab.file=
nfs.kerberos.principal=
nfs.allow.insecure.ports=true
hadoop.fuse.connection.timeout=300
hadoop.fuse.timer.period=5
dfs.namenode.metrics.logger.period.seconds=600
dfs.datanode.metrics.logger.period.seconds=600
dfs.metrics.percentiles.intervals=
dfs.datanode.peer.stats.enabled=false
dfs.datanode.peer.metrics.min.outlier.detection.samples=1000
dfs.datanode.min.outlier.detection.nodes=10
dfs.datanode.slowpeer.low.threshold.ms=5
dfs.datanode.max.nodes.to.report=5
dfs.datanode.outliers.report.interval=30m
dfs.namenode.block-placement-policy.exclude-slow-nodes.enabled=false
dfs.namenode.block-placement.min-blocks-for.write=1
dfs.namenode.max.slowpeer.collect.nodes=5
dfs.namenode.slowpeer.collect.interval=30m
dfs.datanode.fileio.profiling.sampling.percentage=0
dfs.datanode.min.outlier.detection.disks=5
dfs.datanode.slowdisk.low.threshold.ms=20
dfs.datanode.max.disks.to.report=5
dfs.datanode.max.slowdisks.to.exclude=0
hadoop.user.group.metrics.percentiles.intervals=
dfs.encrypt.data.transfer=false
dfs.encrypt.data.transfer.algorithm=
dfs.encrypt.data.transfer.cipher.suites=
dfs.encrypt.data.transfer.cipher.key.bitlength=128
dfs.trustedchannel.resolver.class=
dfs.data.transfer.protection=
dfs.data.transfer.saslproperties.resolver.class=
dfs.journalnode.rpc-address=0.0.0.0:8485
dfs.journalnode.rpc-bind-host=
dfs.journalnode.http-address=0.0.0.0:8480
dfs.journalnode.http-bind-host=
dfs.journalnode.https-address=0.0.0.0:8481
dfs.journalnode.https-bind-host=
dfs.namenode.audit.loggers=default
dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240
dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f
dfs.datanode.round-robin-volume-choosing-policy.additional-available-space=1073741824
dfs.namenode.edits.noeditlogchannelflush=false
dfs.client.cache.drop.behind.writes=
dfs.client.cache.drop.behind.reads=
dfs.client.cache.readahead=
dfs.client.server-defaults.validity.period.ms=3600000
dfs.namenode.observer.enabled=false
dfs.namenode.enable.retrycache=true
dfs.namenode.retrycache.expirytime.millis=600000
dfs.namenode.retrycache.heap.percent=0.03f
dfs.client.mmap.enabled=true
dfs.client.mmap.cache.size=256
dfs.client.mmap.cache.timeout.ms=3600000
dfs.client.mmap.retry.timeout.ms=300000
dfs.client.short.circuit.replica.stale.threshold.ms=1800000
dfs.namenode.caching.enabled=true
dfs.namenode.path.based.cache.block.map.allocation.percent=0.25
dfs.namenode.crm.checklocktime.enable=false
dfs.namenode.crm.maxlocktime.ms=1000
dfs.namenode.crm.sleeptime.ms=300
dfs.datanode.max.locked.memory=0
dfs.datanode.pmem.cache.dirs=
dfs.datanode.pmem.cache.recovery=true
dfs.namenode.list.cache.directives.num.responses=100
dfs.namenode.list.cache.pools.num.responses=100
dfs.namenode.path.based.cache.refresh.interval.ms=30000
dfs.namenode.path.based.cache.retry.interval.ms=30000
dfs.datanode.fsdatasetcache.max.threads.per.volume=4
dfs.datanode.fsdatasetasyncdisk.max.threads.per.volume=4
dfs.cachereport.intervalMsec=10000
dfs.namenode.edit.log.autoroll.multiplier.threshold=0.5
dfs.namenode.edit.log.autoroll.check.interval.ms=300000
dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$
dfs.webhdfs.acl.provider.permission.pattern=^(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?)*$
dfs.webhdfs.socket.connect-timeout=60s
dfs.webhdfs.socket.read-timeout=60s
dfs.client.context=default
dfs.client.read.shortcircuit=false
dfs.client.socket.send.buffer.size=0
dfs.domain.socket.path=
dfs.domain.socket.disable.interval.seconds=600
dfs.client.read.shortcircuit.skip.checksum=false
dfs.client.read.shortcircuit.streams.cache.size=256
dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000
dfs.namenode.audit.log.debug.cmdlist=
dfs.client.use.legacy.blockreader.local=false
dfs.client.read.use.cache.priority=false
dfs.block.local-path-access.user=
dfs.client.domain.socket.data.traffic=false
dfs.namenode.reject-unresolved-dn-topology-mapping=false
dfs.namenode.xattrs.enabled=true
dfs.namenode.fs-limits.max-xattrs-per-inode=32
dfs.namenode.fs-limits.max-xattr-size=16384
dfs.client.slow.io.warning.threshold.ms=30000
dfs.datanode.slow.io.warning.threshold.ms=300
dfs.datanode.processcommands.threshold=2s
dfs.client.deadnode.detection.enabled=false
dfs.client.deadnode.detection.probe.deadnode.threads=10
dfs.client.deadnode.detection.idle.sleep.ms=10000
dfs.client.deadnode.detection.probe.suspectnode.threads=10
dfs.client.deadnode.detection.rpc.threads=20
dfs.client.deadnode.detection.probe.deadnode.interval.ms=60000
dfs.client.deadnode.detection.probe.suspectnode.interval.ms=300
dfs.client.deadnode.detection.probe.connection.timeout.ms=20000
dfs.client.refresh.read-block-locations.ms=0
dfs.client.refresh.read-block-locations.register-automatically=true
dfs.client.refresh.read-block-locations.threads=5
dfs.namenode.lease-recheck-interval-ms=2000
dfs.namenode.max-lock-hold-to-release-lease-ms=25
dfs.namenode.write-lock-reporting-threshold-ms=5000
dfs.namenode.read-lock-reporting-threshold-ms=5000
dfs.namenode.access-control-enforcer-reporting-threshold-ms=1000
dfs.namenode.lock.detailed-metrics.enabled=false
dfs.namenode.fslock.fair=true
dfs.datanode.lock.fair=true
dfs.namenode.startup.delay.block.deletion.sec=0
dfs.datanode.block.id.layout.upgrade.threads=6
dfs.namenode.list.encryption.zones.num.responses=100
dfs.namenode.list.reencryption.status.num.responses=100
dfs.namenode.list.openfiles.num.responses=1000
dfs.namenode.edekcacheloader.interval.ms=1000
dfs.namenode.edekcacheloader.initial.delay.ms=3000
dfs.namenode.reencrypt.sleep.interval=1m
dfs.namenode.reencrypt.batch.size=1000
dfs.namenode.reencrypt.throttle.limit.handler.ratio=1.0
dfs.namenode.reencrypt.throttle.limit.updater.ratio=1.0
dfs.namenode.reencrypt.edek.threads=10
dfs.namenode.inotify.max.events.per.rpc=1000
dfs.user.home.dir.prefix=/user
dfs.datanode.cache.revocation.timeout.ms=900000
dfs.datanode.cache.revocation.polling.ms=500
dfs.storage.policy.enabled=true
dfs.storage.policy.permissions.superuser-only=false
dfs.namenode.legacy-oiv-image.dir=
dfs.namenode.top.enabled=true
dfs.namenode.top.window.num.buckets=10
dfs.namenode.top.num.users=10
dfs.namenode.top.windows.minutes=1,5,25
dfs.webhdfs.ugi.expire.after.access=600000
dfs.namenode.blocks.per.postponedblocks.rescan=10000
dfs.datanode.block-pinning.enabled=false
dfs.client.block.write.locateFollowingBlock.initial.delay.ms=400
dfs.client.block.write.locateFollowingBlock.max.delay.ms=60000
dfs.ha.zkfc.nn.http.timeout.ms=20000
dfs.ha.zkfc.client.ssl.enabled=false
dfs.ha.nn.not-become-active-in-safemode=false
dfs.ha.tail-edits.in-progress=false
dfs.namenode.state.context.enabled=false
dfs.namenode.ec.system.default.policy=RS-6-3-1024k
dfs.namenode.ec.policies.max.cellsize=4194304
dfs.namenode.ec.userdefined.policy.allowed=true
dfs.datanode.ec.reconstruction.stripedread.timeout.millis=5000
dfs.datanode.ec.reconstruction.stripedread.buffer.size=65536
dfs.datanode.ec.reconstruction.threads=8
dfs.datanode.ec.reconstruction.xmits.weight=0.5
dfs.datanode.ec.reconstruction.validation=false
dfs.namenode.quota.init-threads=12
dfs.datanode.transfer.socket.send.buffer.size=0
dfs.datanode.transfer.socket.recv.buffer.size=0
dfs.namenode.upgrade.domain.factor=${dfs.replication}
dfs.datanode.bp-ready.timeout=20
dfs.datanode.cached-dfsused.check.interval.ms=600000
dfs.webhdfs.rest-csrf.enabled=false
dfs.webhdfs.rest-csrf.custom-header=X-XSRF-HEADER
dfs.webhdfs.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD,TRACE
dfs.webhdfs.rest-csrf.browser-useragents-regex=^Mozilla.*,^Opera.*
dfs.xframe.enabled=true
dfs.xframe.value=SAMEORIGIN
dfs.balancer.keytab.enabled=false
dfs.balancer.address=0.0.0.0:0
dfs.balancer.keytab.file=
dfs.balancer.kerberos.principal=
dfs.http.client.retry.policy.enabled=false
dfs.http.client.retry.policy.spec=10000,6,60000,10
dfs.http.client.failover.max.attempts=15
dfs.http.client.retry.max.attempts=10
dfs.http.client.failover.sleep.base.millis=500
dfs.http.client.failover.sleep.max.millis=15000
dfs.namenode.hosts.provider.classname=org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager
datanode.https.port=50475
dfs.namenode.get-blocks.max-qps=20
dfs.namenode.get-blocks.check.operation=true
dfs.balancer.dispatcherThreads=200
dfs.balancer.movedWinWidth=5400000
dfs.balancer.moverThreads=1000
dfs.balancer.max-size-to-move=10737418240
dfs.balancer.getBlocks.min-block-size=10485760
dfs.balancer.getBlocks.size=2147483648
dfs.balancer.block-move.timeout=0
dfs.balancer.max-no-move-interval=60000
dfs.balancer.max-iteration-time=1200000
dfs.block.invalidate.limit=1000
dfs.balancer.service.interval=5m
dfs.balancer.service.retries.on.exception=5
dfs.block.misreplication.processing.limit=10000
dfs.block.placement.ec.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant
dfs.block.replicator.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault
dfs.blockreport.incremental.intervalMsec=0
dfs.checksum.type=CRC32C
dfs.checksum.combine.mode=MD5MD5CRC
dfs.checksum.ec.socket-timeout=3000
dfs.client.block.write.locateFollowingBlock.retries=5
dfs.client.failover.proxy.provider=
dfs.client.failover.random.order=true
dfs.client.failover.resolve-needed=false
dfs.client.failover.resolver.impl=org.apache.hadoop.net.DNSDomainNameResolver
dfs.client.failover.resolver.useFQDN=true
dfs.client.failover.lazy.resolved=false
dfs.client.key.provider.cache.expiry=864000000
dfs.client.max.block.acquire.failures=3
dfs.client.read.prefetch.size=
dfs.client.read.uri.cache.enabled=false
dfs.client.read.short.circuit.replica.stale.threshold.ms=1800000
dfs.client.read.shortcircuit.buffer.size=1048576
dfs.client.short.circuit.num=1
dfs.client.read.striped.threadpool.size=18
dfs.client.replica.accessor.builder.classes=
dfs.client.retry.interval-ms.get-last-block-length=4000
dfs.client.retry.max.attempts=10
dfs.client.retry.policy.enabled=false
dfs.client.retry.policy.spec=10000,6,60000,10
dfs.client.retry.times.get-last-block-length=3
dfs.client.retry.window.base=3000
dfs.client.pipeline.recovery.max-retries=5
dfs.client.socket-timeout=60000
dfs.client.socketcache.capacity=16
dfs.client.socketcache.expiryMsec=3000
dfs.client.test.drop.namenode.response.number=0
dfs.client.hedged.read.threadpool.size=0
dfs.client.hedged.read.threshold.millis=500
dfs.client.write.byte-array-manager.count-limit=2048
dfs.client.write.byte-array-manager.count-reset-time-period-ms=10000
dfs.client.write.byte-array-manager.count-threshold=128
dfs.client.write.byte-array-manager.enabled=false
dfs.client.write.max-packets-in-flight=80
dfs.client.block.reader.remote.buffer.size=512
dfs.content-summary.limit=5000
dfs.content-summary.sleep-microsec=500
dfs.data.transfer.client.tcpnodelay=true
dfs.data.transfer.server.tcpnodelay=true
dfs.data.transfer.max.packet.size=16777216
dfs.datanode.balance.max.concurrent.moves=100
dfs.datanode.data.transfer.bandwidthPerSec=0
dfs.datanode.data.write.bandwidthPerSec=0
dfs.datanode.data.read.bandwidthPerSec=0
dfs.datanode.ec.reconstruct.read.bandwidthPerSec=0
dfs.datanode.ec.reconstruct.write.bandwidthPerSec=0
dfs.datanode.fsdataset.factory=
dfs.datanode.fsdataset.volume.choosing.policy=
dfs.datanode.hostname=
dfs.datanode.lazywriter.interval.sec=60
dfs.datanode.network.counts.cache.max.size=2147483647
dfs.datanode.oob.timeout-ms=1500,0,0,0
dfs.datanode.parallel.volumes.load.threads.num=
dfs.datanode.ram.disk.replica.tracker=
dfs.datanode.restart.replica.expiration=50
dfs.datanode.socket.reuse.keepalive=4000
dfs.datanode.socket.write.timeout=480000
dfs.datanode.sync.behind.writes.in.background=false
dfs.datanode.transferTo.allowed=true
dfs.datanode.fixed.volume.size=false
dfs.datanode.replica.cache.root.dir=
dfs.datanode.replica.cache.expiry.time=5m
dfs.ha.fencing.methods=
dfs.ha.standby.checkpoints=true
dfs.ha.zkfc.port=8019
dfs.ha.allow.stale.reads=false
dfs.journalnode.edits.dir=/tmp/hadoop/dfs/journalnode/
dfs.journalnode.enable.sync=true
dfs.journalnode.sync.interval=120000
dfs.journalnode.edit-cache-size.bytes=
dfs.journalnode.edit-cache-size.fraction=0.5f
dfs.journalnode.kerberos.internal.spnego.principal=
dfs.journalnode.kerberos.principal=
dfs.journalnode.keytab.file=
dfs.batched.ls.limit=100
dfs.ls.limit=1000
dfs.mover.movedWinWidth=5400000
dfs.mover.moverThreads=1000
dfs.mover.retry.max.attempts=10
dfs.mover.keytab.enabled=false
dfs.mover.address=0.0.0.0:0
dfs.mover.keytab.file=
dfs.mover.kerberos.principal=
dfs.mover.max-no-move-interval=60000
dfs.namenode.audit.log.token.tracking.id=false
dfs.namenode.audit.log.with.remote.port=false
dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction=0.6
dfs.namenode.available-space-block-placement-policy.balanced-space-tolerance=5
dfs.namenode.available-space-block-placement-policy.balanced-space-tolerance-limit=100
dfs.namenode.available-space-block-placement-policy.balance-local-node=false
dfs.namenode.available-space-rack-fault-tolerant-block-placement-policy.balanced-space-preference-fraction=0.6
dfs.namenode.available-space-rack-fault-tolerant-block-placement-policy.balanced-space-tolerance=5
dfs.namenode.backup.dnrpc-address=
dfs.namenode.delegation.token.always-use=false
dfs.namenode.edits.asynclogging=true
dfs.namenode.edits.asynclogging.pending.queue.size=4096
dfs.namenode.edits.dir.minimum=1
dfs.namenode.edits.journal-plugin=
dfs.namenode.file.close.num-committed-allowed=0
dfs.namenode.inode.attributes.provider.class=
dfs.namenode.inode.attributes.provider.bypass.users=
dfs.namenode.max-num-blocks-to-log=1000
dfs.namenode.max.op.size=52428800
dfs.namenode.missing.checkpoint.periods.before.shutdown=3
dfs.namenode.name.cache.threshold=10
dfs.namenode.replication.max-streams=2
dfs.namenode.replication.max-streams-hard-limit=4
dfs.namenode.reconstruction.pending.timeout-sec=300
dfs.namenode.excess.redundancy.timeout-sec=3600
dfs.namenode.excess.redundancy.timeout.check.limit=1000
dfs.namenode.stale.datanode.minimum.interval=3
dfs.namenode.remove.dead.datanode.batchnum=10
dfs.namenode.snapshot.capture.openfiles=false
dfs.namenode.snapshot.skip.capture.accesstime-only-change=false
dfs.namenode.snapshotdiff.allow.snap-root-descendant=true
dfs.namenode.snapshotdiff.listing.limit=1000
dfs.namenode.snapshot.max.limit=65536
dfs.namenode.snapshot.filesystem.limit=65536
dfs.namenode.snapshot.skiplist.max.levels=0
dfs.namenode.snapshot.skiplist.interval=10
dfs.storage.policy.satisfier.mode=none
dfs.storage.policy.satisfier.queue.limit=1000
dfs.storage.policy.satisfier.work.multiplier.per.iteration=1
dfs.storage.policy.satisfier.recheck.timeout.millis=60000
dfs.storage.policy.satisfier.self.retry.timeout.millis=300000
dfs.storage.policy.satisfier.retry.max.attempts=3
dfs.storage.policy.satisfier.move.task.retry.max.attempts=3
dfs.storage.policy.satisfier.datanode.cache.refresh.interval.ms=300000
dfs.storage.policy.satisfier.max.outstanding.paths=10000
dfs.storage.policy.satisfier.address=0.0.0.0:0
dfs.storage.policy.satisfier.keytab.file=
dfs.storage.policy.satisfier.kerberos.principal=
dfs.pipeline.ecn=false
dfs.pipeline.slownode=false
dfs.pipeline.congestion.ratio=1.5
dfs.qjournal.accept-recovery.timeout.ms=120000
dfs.qjournal.finalize-segment.timeout.ms=120000
dfs.qjournal.get-journal-state.timeout.ms=120000
dfs.qjournal.new-epoch.timeout.ms=120000
dfs.qjournal.prepare-recovery.timeout.ms=120000
dfs.qjournal.queued-edits.limit.mb=10
dfs.qjournal.select-input-streams.timeout.ms=20000
dfs.qjournal.start-segment.timeout.ms=20000
dfs.qjournal.write-txns.timeout.ms=20000
dfs.qjournal.http.open.timeout.ms=60000
dfs.qjournal.http.read.timeout.ms=60000
dfs.qjournal.parallel-read.num-threads=5
dfs.quota.by.storage.type.enabled=true
dfs.secondary.namenode.kerberos.principal=
dfs.secondary.namenode.keytab.file=
dfs.web.authentication.simple.anonymous.allowed=
dfs.web.ugi=
dfs.webhdfs.netty.high.watermark=65535
dfs.webhdfs.netty.low.watermark=32768
dfs.webhdfs.oauth2.access.token.provider=
dfs.webhdfs.oauth2.client.id=
dfs.webhdfs.oauth2.enabled=false
dfs.webhdfs.oauth2.refresh.url=
ssl.server.keystore.keypassword=
ssl.server.keystore.location=
ssl.server.keystore.password=
ssl.server.truststore.location=
ssl.server.truststore.password=
dfs.disk.balancer.max.disk.throughputInMBperSec=10
dfs.disk.balancer.block.tolerance.percent=10
dfs.disk.balancer.max.disk.errors=5
dfs.disk.balancer.plan.valid.interval=1d
dfs.disk.balancer.enabled=true
dfs.disk.balancer.plan.threshold.percent=10
dfs.namenode.provided.enabled=false
dfs.provided.storage.id=DS-PROVIDED
dfs.provided.aliasmap.class=org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap
dfs.provided.aliasmap.inmemory.batch-size=500
dfs.provided.aliasmap.inmemory.dnrpc-address=
dfs.provided.aliasmap.inmemory.rpc.bind-host=
dfs.provided.aliasmap.inmemory.leveldb.dir=/tmp
dfs.provided.aliasmap.inmemory.enabled=false
dfs.provided.aliasmap.inmemory.server.log=false
dfs.provided.aliasmap.text.delimiter=,
dfs.provided.aliasmap.text.read.file=
dfs.provided.aliasmap.text.codec=
dfs.provided.aliasmap.text.write.dir=
dfs.provided.aliasmap.leveldb.path=
dfs.provided.acls.import.enabled=false
dfs.provided.aliasmap.load.retries=0
dfs.lock.suppress.warning.interval=10s
httpfs.buffer.size=4096
dfs.webhdfs.use.ipc.callq=true
dfs.datanode.disk.check.min.gap=15m
dfs.datanode.disk.check.timeout=10m
dfs.use.dfs.network.topology=true
dfs.net.topology.impl=org.apache.hadoop.hdfs.net.DFSNetworkTopology
dfs.qjm.operations.timeout=60s
dfs.reformat.disabled=false
dfs.namenode.block.deletion.lock.threshold.ms=50
dfs.namenode.block.deletion.unlock.interval.ms=10
dfs.namenode.rpc-address.auxiliary-ports=
dfs.namenode.send.qop.enabled=false
dfs.encrypt.data.overwrite.downstream.derived.qop=false
dfs.encrypt.data.overwrite.downstream.new.qop=
dfs.namenode.blockreport.queue.size=1024
dfs.namenode.storage.dir.perm=700
dfs.namenode.blockreport.max.lock.hold.time=4
dfs.namenode.corrupt.block.delete.immediately.enabled=true
dfs.journalnode.edits.dir.perm=700
dfs.journalnode.handler.count=5
dfs.namenode.lease-hard-limit-sec=1200
dfs.namenode.gc.time.monitor.enable=true
dfs.namenode.gc.time.monitor.observation.window.ms=1m
dfs.namenode.gc.time.monitor.sleep.interval.ms=5s
dfs.permissions.allow.owner.set.quota=false
dfs.protected.subdirectories.enable=false
dfs.storage.default.policy=HOT
dfs.datanode.same-disk-tiering.enabled=false
dfs.datanode.reserve-for-archive.default.percentage=0.0
dfs.datanode.same-disk-tiering.capacity-ratio.percentage=
dfs.balancer.getBlocks.hot-time-interval=0
dfs.datanode.directoryscan.max.notify.count=5
dfs.datanode.nameservices.resolution-enabled=false
dfs.datanode.nameservices.resolver.impl=
dfs.client.mark.slownode.as.badnode.threshold=10
dfs.datanode.lockmanager.trace=false
dfs.client.fsck.connect.timeout=60000ms
dfs.client.fsck.read.timeout=60000ms
dfs.client.output.stream.uniq.default.key=DEFAULT
dfs.client.congestion.backoff.mean.time=5000
dfs.client.congestion.backoff.max.time=50000
dfs.client.rbf.observer.read.enable=false
